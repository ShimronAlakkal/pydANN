{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pydANN ( python deep Artificial Neural Network )\n",
    "\n",
    "is a free and open source python library to implement the Machine Learning algorithm of neural networks\n",
    "The network can be as simple as a sinle layer perceptron net or a multi-layer deep neural net.\n",
    "THe design and modifications of this library is posted [here](https://www.github.com/ShimronAlakkal)\n",
    "\n",
    "\n",
    "### 1 - Packages\n",
    "\n",
    "These are some of the most important packages that you're going to need in order to use ***```pydANN```***\n",
    "\n",
    "- [numpy](www.numpy.org) (or numeric python) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs -- which are optional -- in Python.\n",
    "- [pickle](https://docs.python.org/3/library/pickle.html) is the library pydANN uses to save your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tools (functions) that are called inside the activation layer.\n",
    "To specify the activation function for a layer use `activation_specific = f` with `addHL()`,  where `f` is a list, of length of hidden layers + 1, and each index with a custom function name.\n",
    "If there is a mismatch in the input activation_specifics, the model is going to auto adjust the activation with the last ones from your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-1 * Z))\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    return np.maximum(0.1*Z)\n",
    "\n",
    "def sigmoid_d(Z):\n",
    "    return sigmoid(Z) * ( 1 - sigmoid(Z) ) \n",
    "\n",
    "def relu_d(Z):\n",
    "    Z[Z >= 0] = 1\n",
    "    Z[Z < 0]  = 0\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden_Layers = [3,2]\n",
    "        self.total_layers = []\n",
    "        self.learning_rate = 0\n",
    "        self.epoch = 0\n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        self.Z = {}\n",
    "        self.A = {}\n",
    "        self.activation_functions = []\n",
    "        self.costs = [0]\n",
    "        self.dW = {}\n",
    "        self.db = {}\n",
    "        self.dZ = {}\n",
    "        self.dA = {}\n",
    "        self.lr_change = []\n",
    "        \n",
    "    \n",
    "    \n",
    "    def add_hl(self,hl,activations ):\n",
    "        self.hidden_Layers.clear()\n",
    "        self.hidden_Layers = hl\n",
    "        \n",
    "        # settingf the activations\n",
    "        if len(activations) == len(hl)+1:\n",
    "            self.activation_functions = activations\n",
    "        else:\n",
    "            print('Passed activations should be 1 more than the HL length \\n recall the function to override HL . Re-adjusting the layers')\n",
    "            if len(activations) < len(hl+1):\n",
    "                for i in range (len(hl+1) - len(activations)):\n",
    "                    self.activation_functions.append(self.activation_functions[-1])\n",
    "                \n",
    "        \n",
    "    def dispose_model(self):\n",
    "        self.total_layers.clear()\n",
    "        self.hidden_Layers.clear()\n",
    "        self.costs.clear()\n",
    "        self.lr_change.clear()\n",
    "        self.Z.clear()\n",
    "        self.W.clear()\n",
    "        self.b.clear()\n",
    "        self.db.clear()\n",
    "        self.dW.clear()\n",
    "        self.dZ.clear()\n",
    "        self.dA.clear()\n",
    "            \n",
    "    def register_training_data(self,train_x,train_y):\n",
    "        self.total_layers.clear()\n",
    "        self.total_layers.append(train_x.shape[0])\n",
    "        for i in self.hidden_Layers:\n",
    "            self.total_layers.append(i)\n",
    "       \n",
    "        # network structure\n",
    "        self.total_layers.append(train_y.shape[0])\n",
    "        print(f\"Network structure update :{self.total_layers}\\n feature(s) : {self.total_layers[0]} \\n label(s) : {self.total_layers[-1]} \\n hidden layers : {self.hidden_Layers}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_Params(self,verbose = False):\n",
    "        \n",
    "        # creating the weights and biases with seed(1)\n",
    "        np.random.seed(144)\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            \n",
    "            self.W['W'+str(i)] = np.random.randn(self.total_layers[i],self.total_layers[i-1]) * 0.01\n",
    "            self.b['b'+str(i)] = np.random.randn(self.total_layers[i],1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('shape of weight(s) initialized : \\n ')\n",
    "            for i in self.W.values():\n",
    "                print(i.shape)\n",
    "            print('shape of bias(es) initialized : \\n ')\n",
    "            for i in self.b.values():\n",
    "                print(i.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forePropagate(self,train_x):\n",
    "        self.A['A0'] = train_x\n",
    "        a = self.activation_functions\n",
    "        \n",
    "        # populating Z and A with data\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            \n",
    "            # the formula for fore-propagation is z = W.X + b\n",
    "            self.Z[ 'Z'+str(i) ] = np.dot( self.W['W'+str(i)] , self.A['A'+str(i-1)] ) + self.b['b'+str(i)]\n",
    "          \n",
    "            # populating the activation dictionary with index values\n",
    "            \n",
    "            if a[i-1] == 'relu':\n",
    "                self.A['A'+str(i)] = relu( self.Z['Z'+str(i)] )\n",
    "            elif a[i-1] == 'sigmoid': \n",
    "                self.A['A'+str(i)] = sigmoid( self.Z['Z'+str(i)] )\n",
    "            else:\n",
    "                self.A['A'+str(i)] = leaky_relu( self.Z['Z'+str(i)] )\n",
    "                \n",
    "    \n",
    "    def cost_calc(self,Y,loss_function ):\n",
    "        \n",
    "        # the `m` used in cost functions represent the total number of training examples\n",
    "        if loss_function in ['mse','MSE']:\n",
    "            \n",
    "            # use mean squared error function     \n",
    "            loss = ( 1 /  Y.shape[1]) * ( np.sum(np.square (  Y - self.A[ 'A'+str(len(self.total_layers)-1)])))\n",
    "            cost = np.squeeze(loss)\n",
    "         \n",
    "            self.costs.append(cost)\n",
    "            \n",
    "            \n",
    "        else : #['rmse','RMSE']:\n",
    "            \n",
    "            # use the root mean squared function\n",
    "            loss = np.sqrt( ( 1 / Y.shape[1]) * ( np.sum(np.square (  Y - self.A[ 'A'+str(len(self.total_layers)-1)])))) \n",
    "            cost = np.squeeze(loss)\n",
    "            \n",
    "            self.costs.append(cost)\n",
    "        \n",
    "#         elif loss_function in ['mae','MAE']:\n",
    "            \n",
    "#             use the mean absolute error function here \n",
    "#             self.costs.append( 1 / Y.shape[1] * (np.sum(  )) )  # you're going to have to do modulus here\n",
    "            \n",
    "#         else:\n",
    "            \n",
    "#             # use binary cross entropy\n",
    "#             self.costs.append( np.squeeze(-1 * np.sum( np.multiply( Y ,np.log(self.A[ 'A'+str(len(self.total_layers)-1)]) ) +\n",
    "#                                                         np.multiply( (1-Y),np.log(1-self.A[ 'A'+str(len(self.total_layers)-1)]) ) ) / Y.shape[1] ) )\n",
    "            \n",
    "            \n",
    "            \n",
    "    def back_prop(self,Y):\n",
    "        \n",
    "        # compute dA final layer \n",
    "        self.dA['dA'+str(len(self.total_layers)-1)] =  (-1 * np.divide(Y,self.A['A'+str(len(self.total_layers)-1)])) - np.divide(1-Y, 1-self.A['A'+str(len(self.total_layers)-1)])\n",
    "        \n",
    "        \n",
    "        # check for the final layer activation_func\n",
    "        if self.activation_functions[-1] == 'sigmoid':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , sigmoid_d(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        elif self.activation_functions[-1] == 'relu':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , relu_d(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "#         else:\n",
    "#             self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , leaky_relu(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        \n",
    "        # get dW final layer\n",
    "        self.dW['dW'+str(len(self.total_layers)-1)] = ( 1 / Y.shape[1] ) * np.dot( self.dZ['dZ'+str(len(self.total_layers)-1)] , self.A['A'+str(len(self.total_layers)-2)].T )\n",
    "        \n",
    "        # get db final layer \n",
    "        self.db['db'+str(len(self.total_layers)-1)] = (1/Y.shape[1]) * np.sum(self.dZ['dZ'+str(len(self.total_layers)-1)],axis = 1, keepdims = True)\n",
    "        \n",
    "        self.dA['dA'+str(len(self.total_layers)-2)] = np.dot(self.W['W'+str(len(self.total_layers)-1)].T , self.dZ['dZ'+str(len(self.total_layers)-1)] )\n",
    "        \n",
    "        \n",
    "        # loop over the number of hidden layers + 1 in the network in reverse and find weights and biases for them\n",
    "        for i in reversed(range(1,len(self.total_layers)-1)):\n",
    "            \n",
    "            # check for DZ and get it done\n",
    "            if self.activation_functions[i] == 'sigmoid':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , sigmoid_d(self.Z['Z'+str(i)]) )\n",
    "            elif self.activation_functions[i] == 'relu':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , relu_d(self.Z['Z'+str(i)]) )\n",
    "#             else:\n",
    "#                 self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , leaky_relu_d(self.Z['Z'+str(i)]) )\n",
    "      \n",
    "            \n",
    "            self.dW['dW'+str(i)] = np.dot(self.dZ['dZ'+str(i)], self.A['A'+str(i - 1)].T) / Y.shape[1]\n",
    "            self.db['db'+str(i)] = np.sum(self.dZ['dZ'+str(i)], axis = 1, keepdims = True) / Y.shape[1]\n",
    "            self.dA['dA'+str(i - 1)] = np.dot(self.W['W'+str(i)].T, self.dZ['dZ'+str(i)])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def param_update(self):\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            self.W['W'+str(i)] -= self.learning_rate * self.dW['dW'+str(i)]\n",
    "            self.b['b'+str(i)] -= self.learning_rate * self.db['db'+str(i)]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def fit(self,xtrain,ytrain,epoch = 50, learning_rate = 0.01, verbose = 0,decay = True, decay_iter = 5, decay_rate = 0.9, stop_decay_counter = 100, loss_function = 'mse'):\n",
    "#         tx = xtrain.T\n",
    "#         ty = ytrain.T\n",
    "        self.learning_rate = learning_rate\n",
    "        self.costs.clear()\n",
    "        self.lr_change.clear()\n",
    "        \n",
    "        self.register_training_data(xtrain,ytrain)\n",
    "        self.init_Params()\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            \n",
    "            self.forePropagate(tx)\n",
    "            \n",
    "            self.cost_calc(ty,loss_function = loss_function)\n",
    "            \n",
    "            self.back_prop(ty)\n",
    "            \n",
    "            self.param_update()\n",
    "            \n",
    "            self.lr_change.append(self.learning_rate)\n",
    "            \n",
    "            if decay and stop_decay_counter > 0 and i % decay_iter == 0:\n",
    "                self.learning_rate = decay_rate * self.learning_rate\n",
    "                stop_decay_counter -= 1\n",
    "                self.lr_change.append(self.learning_rate)\n",
    "                \n",
    "            if verbose and i % verbose == 0 and str( self.costs[-1] ) != 'nan':\n",
    "                \n",
    "                print(f'epoch {i} : \\t cost = {self.costs[-1]}')\n",
    "                print(f'learning rate / alpha \\t{self.learning_rate}\\n')\n",
    "            \n",
    "        print(f'epoch {epoch}:\\t  cost = {self.costs[-1]}')\n",
    "        print(f'learning rate / alpha \\t{self.learning_rate}\\n')\n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "    \n",
    "    def predict(self,xtest):\n",
    "        \n",
    "        # we fore prop at first and return the last A\n",
    "        self.forePropagate(xtest)\n",
    "        \n",
    "        return self.A['A'+str(len(self.total_layers)-1)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def mse_model_eval(self,ytest,ypreds):\n",
    "        \n",
    "        # compute the Mean Squared Error with y-preds and y-test\n",
    "        try:\n",
    "            loss = ( 1 / ypreds.shape[1]) * ( np.sum(np.square (  ypreds - self.A[ 'A'+str(len(self.total_layers)-1)])))\n",
    "            print(np.squeeze(loss))\n",
    "        except:\n",
    "            print('Please check the indices and re-try')\n",
    "            \n",
    "            \n",
    "    \n",
    "    def plot_cost_to_epoch(self):\n",
    "        self.costs = [self.costs[x] for x in range(1,len(self.costs)) if str(self.costs[x]) != 'nan' ]\n",
    "        plt.plot(self.costs, color = 'r')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_lrc_to_epoch(self):\n",
    "        plt.plot(self.lr_change)\n",
    "        plt.show;\n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_model(self,file = 'pydann_model.dat'):\n",
    "        \n",
    "        model = {'w':self.W,'b':self.b,'lr':self.learning_rate,'actvns':self.activation_functions,\n",
    "                        'hl':self.hidden_Layers,'tl':self.total_layers}\n",
    "        with open(file,'wb') as file :\n",
    "            pickle.dump(model,file)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def use_model(self,path):\n",
    "        try:\n",
    "            with open(path,'rb') as file:\n",
    "                model = pickle.load(file)\n",
    "                self.W = model['w']\n",
    "                self.b = model['b']\n",
    "                self.activation_functions = model['actvns']\n",
    "                self.total_layers = model['tl']\n",
    "                self.hidden_Layers = model['hl']\n",
    "                self.learning_rate = model['lr']\n",
    "        except:\n",
    "            print(f\"unable to open {path}\\n check if you've added the file extension(.dat) with the file path\")\n",
    "    \n",
    "    \n",
    "    def auto_model_setup(self,seed):\n",
    "        pass\n",
    "    \n",
    "    def help(self):\n",
    "        print('''\n",
    "        \\033[1m \n",
    "        ANN model Help\n",
    "        \n",
    "        Inorder to make an Artificial Neural Network model using pydANN, intanciate the class ann()\n",
    "        \\033[0m \n",
    "        model = ann()\n",
    "        \n",
    "        \\033[1m\n",
    "        Inorder to add hidden layers to the model use the method add_hl( hidden_Layers_list , activations_for_each_layer + 1 )\n",
    "        \\033[0m\n",
    "        \n",
    "        \n",
    "        # we are now going to add hidden layers to the model \n",
    "\n",
    "        model.add_hl( [ 2,5,5 ] , activation_functions = ['relu', 'relu' ,'relu' ,'sigmoid'] )\n",
    "\n",
    "        # the above line adds to the model three hidden layers each of 2, 5 and 5 nodes, respectively. \n",
    "        # The activation_functions should be 1 more in length so as to have one for each hl and an activation for the output layer\n",
    "        \n",
    "        \\033[1m\n",
    "        To train the model, use the fit( xtrain, ytrain, epoch = 50, learning_rate = 0.01, verbose = 0,decay = True, decay_iter = 5, decay_rate = 0.9, stop_decay_counter = 100, loss_function = 'mse' ) method\n",
    "        \\033[0m\n",
    "        \n",
    "        # you can also register the training data before you train the model.\n",
    "\n",
    "        model.fit( train_x , y_train )  # this is the basic implementation of the method without any alteration\n",
    "        epoch : The number of iterations to train the model\n",
    "         \n",
    "         \n",
    "        \\033[1m \n",
    "        learning_rate : \\033[0m The value with which the algorithm optimizes weights and biases\n",
    "\\033[1m\n",
    "        verbose :\\033[0m When put 0, verbose is False ( There won't be data printing ) if set to another value other than 0, then the data is printed after each verbose interval\n",
    "\\033[1m\n",
    "        decay :\\033[0m This value optimizes the learning_rate when set to True\n",
    "\\033[1m\n",
    "        loss_function :\\033[0m The function using which the loss is calculated. Loss function option : 'mse' , 'rmse'\n",
    "\\033[1m\n",
    "        decay_rate :\\033[0m The fractional value with which the learning rate is altered\n",
    "\\033[1m\n",
    "        To plot the change in the cost(s) after each epoch, use plot_cost_to_epoch()\n",
    "               \\033[0m\n",
    "\n",
    "        model.plot_cost_to_epoch()\n",
    "        \n",
    "        \\033[1m\n",
    "        To plot the change in learning_rate after each epoch, use plot_lrc_to_epoch()\n",
    "        model.plot_lrc_to_epoch()\n",
    "        \n",
    "                \\033[0m\n",
    "\n",
    "        \\033[1m \n",
    "        To predict on test data, use the function predict( xtest )\n",
    "        \\033[0m\n",
    "        y_predictions = model.predict( xtest )\n",
    "        \n",
    "        \\033[1m\n",
    "        To evaluate the model base on Mean Squared Error, use mse_model_eval( ytest,ypreds )\n",
    "        \\033[0m\n",
    "        model.mse_model_eval( ytest,ypreds )\n",
    "        \n",
    "        \\033[1m\n",
    "        To save the current trained model use method save_model( file = 'pydann_model.dat' )\n",
    "        \\033[0m\n",
    "        \n",
    "        # 'file' is the name of the file in which the model would be saved and it has to be a .dat file\n",
    "        model.save_model()     # to save the model as pydann_model.dat just call the function like this\n",
    "\n",
    "        # to customize the name of the file in which the model should be saved , change 'file'\n",
    "        model.save_model( file = 'model.dat' )\n",
    "       \n",
    "       \\033[1m\n",
    "       To use the saved model use method use_model( path )\n",
    "        \\033[0m\n",
    "        # path is the file path ( directory ) to the saved model\n",
    "        model.use_model( )\n",
    "        \n",
    "        \\033[1m\n",
    "        To dispose the model and clear memory use the dispose_model() method\n",
    "        \\033[0m\n",
    "        model.dispose_model() \n",
    "        \n",
    "        ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the model on meaningless data which might make the whole thing go wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ann()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_hl([8,6,6,3],activations = ['relu','sigmoid','relu','relu','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13234)\n",
    "tx = np.random.random((3,100))\n",
    "ty = np.random.random((1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network structure update :[3, 8, 6, 6, 3, 1]\n",
      " feature(s) : 3 \n",
      " label(s) : 1 \n",
      " hidden layers : [8, 6, 6, 3]\n"
     ]
    }
   ],
   "source": [
    "model.register_training_data(np.random.random((3,100)),np.random.random((1,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.01298571, -0.00092539,  0.00070074],\n",
      "       [ 0.01855052,  0.0137026 , -0.0018258 ],\n",
      "       [-0.01170023,  0.01027954, -0.00834468],\n",
      "       [ 0.00187793, -0.00648421,  0.00829418],\n",
      "       [-0.01505343,  0.00870126,  0.01223903],\n",
      "       [-0.02231902,  0.00028256,  0.00310356],\n",
      "       [ 0.00554466,  0.00293178,  0.00538658],\n",
      "       [ 0.01067838, -0.01733088, -0.00193664]]), 'W2': array([[ 0.00211083,  0.00659727, -0.00546553,  0.00159969, -0.00299756,\n",
      "         0.00271352,  0.01249906, -0.01398527],\n",
      "       [ 0.0023012 , -0.00224214, -0.00360311, -0.00192167,  0.01998439,\n",
      "         0.01085534, -0.01633774, -0.01309612],\n",
      "       [-0.01763754, -0.01216804, -0.00234821, -0.00437128, -0.00292036,\n",
      "         0.00276368, -0.00802196, -0.00089666],\n",
      "       [-0.00299194, -0.00312767, -0.00597397, -0.00332675,  0.00213999,\n",
      "         0.01406547,  0.00250701,  0.00974168],\n",
      "       [-0.00032676,  0.00814333, -0.00787541,  0.00264752, -0.0107492 ,\n",
      "        -0.0042068 , -0.00892893,  0.00927921],\n",
      "       [ 0.00265021,  0.01508603, -0.00075199,  0.00793893,  0.00882451,\n",
      "        -0.00211563, -0.00311329,  0.00187612]]), 'W3': array([[-0.00400013, -0.01134744, -0.01079014, -0.01456128, -0.010018  ,\n",
      "         0.00543085],\n",
      "       [ 0.00731462,  0.01034002,  0.01418882, -0.02300068,  0.00343181,\n",
      "         0.00693116],\n",
      "       [ 0.01093519,  0.00369791,  0.00107428, -0.01922627, -0.00240342,\n",
      "        -0.02004464],\n",
      "       [ 0.00503542,  0.00449346, -0.0149607 , -0.00149875,  0.01450884,\n",
      "         0.01265724],\n",
      "       [ 0.00850734,  0.01164604,  0.00904421, -0.00258371,  0.01060894,\n",
      "        -0.00439105],\n",
      "       [ 0.00441122,  0.01085463, -0.00656174,  0.00156292, -0.01027357,\n",
      "        -0.00144797]]), 'W4': array([[-0.00262879, -0.00305914, -0.00164227,  0.00788528,  0.00271657,\n",
      "         0.00334115],\n",
      "       [ 0.02246307, -0.00121998, -0.00968551, -0.02897117,  0.00577106,\n",
      "         0.00042441],\n",
      "       [ 0.00417509,  0.02080935, -0.01764104,  0.00332555, -0.00815445,\n",
      "        -0.00516989]]), 'W5': array([[ 0.0133718 , -0.00818097,  0.0152592 ]])}\n",
      "\n",
      "{'b1': array([[-0.19915519],\n",
      "       [-1.13385466],\n",
      "       [ 2.21669181],\n",
      "       [-1.17293195],\n",
      "       [ 0.55861575],\n",
      "       [-0.7900395 ],\n",
      "       [ 0.80141421],\n",
      "       [-0.03746725]]), 'b2': array([[ 0.81552061],\n",
      "       [-0.32649909],\n",
      "       [-0.12450633],\n",
      "       [-0.33937231],\n",
      "       [ 1.1005343 ],\n",
      "       [-1.71636314]]), 'b3': array([[ 1.05071634],\n",
      "       [-0.73866615],\n",
      "       [-0.56361126],\n",
      "       [ 0.91855338],\n",
      "       [-0.07438515],\n",
      "       [ 0.05651145]]), 'b4': array([[-0.89634258],\n",
      "       [-0.04324785],\n",
      "       [ 0.0351752 ]]), 'b5': array([[-0.74578468]])}\n"
     ]
    }
   ],
   "source": [
    "model.forePropagate(tx)\n",
    "\n",
    "print(model.W)\n",
    "print()\n",
    "print(model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cost_calc(ty,loss_function = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.back_prop(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dW5': array([[ 0.        ,  0.        , -0.02062989]]), 'dW4': array([[-1.38032712e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "        -1.25046763e-03,  0.00000000e+00, -7.24257207e-05],\n",
      "       [ 1.02314859e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "         9.26892015e-04,  0.00000000e+00,  5.36845742e-05],\n",
      "       [-1.90858924e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "        -1.72903149e-03,  0.00000000e+00, -1.00143618e-04]]), 'dW3': array([[ 1.25889114e-05,  7.57461018e-06,  8.46402566e-06,\n",
      "         7.51765565e-06,  1.35354197e-05,  2.77118705e-06],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [-3.16515108e-05, -1.90443676e-05, -2.12805692e-05,\n",
      "        -1.89011704e-05, -3.40312574e-05, -6.96742190e-06],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 3.84192312e-06,  2.31164309e-06,  2.58307767e-06,\n",
      "         2.29426153e-06,  4.13078148e-06,  8.45719482e-07]]), 'dW2': array([[ 0.00000000e+00,  0.00000000e+00, -6.15482471e-07,\n",
      "         0.00000000e+00, -1.56365327e-07,  0.00000000e+00,\n",
      "        -2.25042308e-07,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -1.99473079e-06,\n",
      "         0.00000000e+00, -5.06767856e-07,  0.00000000e+00,\n",
      "        -7.29344606e-07,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00]]), 'dW1': array([[-3.56666278e-11, -3.14932392e-11, -3.95705875e-11],\n",
      "       [-8.40044618e-10, -7.41701790e-10, -9.30494735e-10],\n",
      "       [ 3.78780518e-10,  3.33231041e-10,  4.20026528e-10],\n",
      "       [-2.52291949e-10, -2.22499464e-10, -2.79988680e-10],\n",
      "       [ 1.20037808e-09,  1.05856812e-09,  1.33015960e-09],\n",
      "       [ 3.19984404e-10,  2.82862787e-10,  3.55407098e-10],\n",
      "       [ 4.80435519e-10,  4.24196880e-10,  5.32922418e-10],\n",
      "       [-5.51203798e-10, -4.86561304e-10, -6.11399915e-10]])}\n"
     ]
    }
   ],
   "source": [
    "print(model.dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.param_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network structure update :[3, 8, 6, 6, 3, 1]\n",
      " feature(s) : 3 \n",
      " label(s) : 1 \n",
      " hidden layers : [8, 6, 6, 3]\n",
      "epoch 0 : \t cost = 0.11466460167592583\n",
      "learning rate / alpha \t0.00025\n",
      "\n",
      "epoch 50:\t  cost = 0.11458232791021854\n",
      "learning rate / alpha \t9.5367431640625e-10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(tx,ty,verbose=100,decay=True,decay_rate=0.25,learning_rate = 0.001,epoch=50,loss_function='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.plot_cost_to_epoch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_lrc_to_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(324)\n",
    "testx = np.random.random((3,100))\n",
    "testy = np.random.random((1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = model.predict(testx)\n",
    "yp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mse_model_eval(testy,yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_model('pydann_model.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
