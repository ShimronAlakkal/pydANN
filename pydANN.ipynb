{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pydANN ( python deep Artificial Neural Network )\n",
    "\n",
    "is a free and open source python library to implement the Machine Learning algorithm of neural networks\n",
    "The network can be as simple as a sinle layer perceptron net or a multi-layer deep neural net.\n",
    "THe design and modifications of this library is posted [here](https://www.github.com/ShimronAlakkal)\n",
    "\n",
    "\n",
    "### 1 - Packages\n",
    "\n",
    "These are some of the most important packages that you're going to need in order to use ***```pydANN```***\n",
    "\n",
    "- [numpy](www.numpy.org) (or numeric python) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs -- which are optional -- in Python.\n",
    "- [pickle](https://docs.python.org/3/library/pickle.html) is the library pydANN uses to save your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tools (functions) that are called inside of the activation layer.\n",
    "To specify the activation function for a layer use `activation_specific = f` with `addHL()`,  where `f` is a list, of length of hidden layers + 1, and each index with a custom function name.\n",
    "If there is a mismatch in the input activation_specifics, the model is going to auto adjust the activation with the last ones from your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / 1 + np.exp(-Z)\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    return np.maximum(0.1*Z)\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * ( 1 - sigmoid(Z) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden_Layers = [3,2]\n",
    "        self.total_layers = []\n",
    "        self.learning_rate = 0.001\n",
    "        self.epoch = 0\n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        self.Z = {}\n",
    "        self.A = {}\n",
    "        self.activation_functions = []\n",
    "        self.costs = [0]\n",
    "        self.dW = {}\n",
    "        self.db = {}\n",
    "        self.dZ = {}\n",
    "        self.dA = {}\n",
    "        self.lr_change = []\n",
    "        \n",
    "    \n",
    "    \n",
    "    def add_hl(self,hl,activations ):\n",
    "        self.hidden_Layers.clear()\n",
    "        self.hidden_Layers = hl\n",
    "        \n",
    "        # settingf the activations\n",
    "        if len(activations) == len(hl)+1:\n",
    "            self.activation_functions = activations\n",
    "        else:\n",
    "            print('Passed activations should be 1 more than the HL length \\n recall the function to override HL')\n",
    "                \n",
    "        \n",
    "    def clear_instance_data():\n",
    "        self.total_layers.clear()\n",
    "        self.hidden_Layers.clear()\n",
    "        self.Z.clear()\n",
    "        self.W.clear()\n",
    "        self.b.clear()\n",
    "            \n",
    "            \n",
    "    def register_training_data(self,train_x,train_y):\n",
    "        self.total_layers.clear()\n",
    "        self.total_layers.append(train_x.shape[0])\n",
    "        for i in self.hidden_Layers:\n",
    "            self.total_layers.append(i)\n",
    "       \n",
    "        # network structure\n",
    "        self.total_layers.append(train_y.shape[0])\n",
    "        print(f\"Network structure update :{self.total_layers}\\n feature(s) : {self.total_layers[0]} \\n label(s) : {self.total_layers[-1]} \\n hidden layers : {self.hidden_Layers}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_Params(self,verbose = False):\n",
    "        \n",
    "        # creating the weights and biases with seed(1)\n",
    "        np.random.seed(144)\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            \n",
    "            self.W['W'+str(i)] = np.random.randn(self.total_layers[i],self.total_layers[i-1]) * 0.01\n",
    "            self.b['b'+str(i)] = np.random.randn(self.total_layers[i],1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('shape of weight(s) initialized : \\n ')\n",
    "            for i in self.W.values():\n",
    "                print(i.shape)\n",
    "            print('shape of bias(es) initialized : \\n ')\n",
    "            for i in self.b.values():\n",
    "                print(i.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forePropagate(self,train_x):\n",
    "        self.A['A0'] = train_x\n",
    "        a = self.activation_functions\n",
    "        \n",
    "        # populating Z and A with data\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            \n",
    "            # the formula for fore-propagation is z = W.X + b\n",
    "            self.Z[ 'Z'+str(i) ] = np.dot( self.W['W'+str(i)] , self.A['A'+str(i-1)] ) + self.b['b'+str(i)]\n",
    "          \n",
    "            # populating the activation dictionary with index values\n",
    "            \n",
    "            if a[i-1] == 'relu':\n",
    "                self.A['A'+str(i)] = relu( self.Z['Z'+str(i)] )\n",
    "            elif a[i-1] == 'sigmoid': \n",
    "                self.A['A'+str(i)] = sigmoid( self.Z['Z'+str(i)] )\n",
    "            elif a[i-1] == 'sigmoid_d':\n",
    "                self.A['A'+str(i)] = sigmoid_derivative( self.Z['Z'+str(i)] )\n",
    "            else:\n",
    "                self.A['A'+str(i)] = leaky_relu( self.Z['Z'+str(i)] )\n",
    "                \n",
    "    \n",
    "    def cost_calc(self,Y,loss_function = 'mse'):\n",
    "        \n",
    "        # the `m` used in cost functions represent the total number of training examples\n",
    "        if loss_function in ['mse','MSE']:\n",
    "            \n",
    "            # use mean squared error function     \n",
    "            loss = ( 1 / ( 2 * Y.shape[1])) * ( np.sum(np.square (  Y - self.A[ 'A'+str(len(self.total_layers)-1)])))\n",
    "            cost = np.squeeze(loss)\n",
    "         \n",
    "            self.costs.append(cost)\n",
    "            \n",
    "            \n",
    "        else : #['rmse','RMSE']:\n",
    "            \n",
    "            # use the root mean squared function\n",
    "            loss = np.sqrt( ( 1 / Y.shape[1]) * ( np.sum(np.square (  Y - self.A[ 'A'+str(len(self.total_layers)-1)])))) \n",
    "            cost = np.squeeze(loss)\n",
    "            \n",
    "            self.costs.append(cost)\n",
    "        \n",
    "#         elif loss_function in ['mae','MAE']:\n",
    "            \n",
    "#             use the mean absolute error function here \n",
    "#             self.costs.append( 1 / Y.shape[1] * (np.sum(  )) )  # you're going to have to do modulus here\n",
    "            \n",
    "#         else:\n",
    "            \n",
    "#             # use binary cross entropy\n",
    "#             self.costs.append( np.squeeze(-1 * np.sum( np.multiply( Y ,np.log(self.A[ 'A'+str(len(self.total_layers)-1)]) ) +\n",
    "#                                                         np.multiply( (1-Y),np.log(1-self.A[ 'A'+str(len(self.total_layers)-1)]) ) ) / Y.shape[1] ) )\n",
    "            \n",
    "            \n",
    "            \n",
    "    def back_prop(self,Y):\n",
    "        \n",
    "        # compute dA final layer \n",
    "        self.dA['dA'+str(len(self.total_layers)-1)] = -1 * np.divide(Y,self.A['A'+str(len(self.total_layers)-1)]) + np.divide(1-Y, 1-self.A['A'+str(len(self.total_layers)-1)])\n",
    "        \n",
    "        \n",
    "        # check for the final layer activation_func\n",
    "        if self.activation_functions[-1] == 'sigmoid':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , sigmoid(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        elif self.activation_functions[-1] == 'sigmoid_d':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , sigmoid_derivative(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        elif self.activation_functions[-1] == 'relu':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , relu(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        else:\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , leaky_relu(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        \n",
    "        # get dW final layer\n",
    "        self.dW['dW'+str(len(self.total_layers)-1)] = ( 1 / Y.shape[1] ) * np.dot( self.dZ['dZ'+str(len(self.total_layers)-1)] , self.A['A'+str(len(self.total_layers)-2)].T )\n",
    "        \n",
    "        # get db final layer \n",
    "        self.db['db'+str(len(self.total_layers)-1)] = (1/Y.shape[1]) * np.sum(self.dZ['dZ'+str(len(self.total_layers)-1)],axis = 1, keepdims = True)\n",
    "        \n",
    "        self.dA['dA'+str(len(self.total_layers)-2)] = np.dot(self.W['W'+str(len(self.total_layers)-1)].T , self.dZ['dZ'+str(len(self.total_layers)-1)] )\n",
    "        \n",
    "        \n",
    "        # loop over the number of hidden layers + 1 in the network in reverse and find weights and biases for them\n",
    "        for i in reversed(range(1,len(self.total_layers)-1)):\n",
    "            \n",
    "            # check for DZ and get it done\n",
    "            if self.activation_functions[i] == 'sigmoid':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , sigmoid(self.Z['Z'+str(i)]) )\n",
    "            elif self.activation_functions[i] == 'sigmoid_d':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , sigmoid_derivative(self.Z['Z'+str(i)]) )\n",
    "            elif self.activation_functions[i] == 'relu':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , relu(self.Z['Z'+str(i)]) )\n",
    "            else:\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , leaky_relu(self.Z['Z'+str(i)]) )\n",
    "      \n",
    "            \n",
    "            self.dW['dW'+str(i)] = np.dot(self.dZ['dZ'+str(i)], self.A['A'+str(i - 1)].T) / Y.shape[1]\n",
    "            self.db['db'+str(i)] = np.sum(self.dZ['dZ'+str(i)], axis = 1, keepdims = True) / Y.shape[1]\n",
    "            self.dA['dA'+str(i - 1)] = np.dot(self.W['W'+str(i)].T, self.dZ['dZ'+str(i)])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def param_update(self):\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            self.W['W'+str(i)] -= self.learning_rate * self.dW['dW'+str(i)]\n",
    "            self.b['b'+str(i)] -= self.learning_rate * self.db['db'+str(i)]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def fit(self,xtrain,ytrain,epoch = 5000, learning_rate = 0.0001,lr_decay_rate = 0.9, verbose = 0,lr_decay = False,lr_decay_epoch = 10,decay_stop = 50, loss_function = 'mse'):\n",
    "#         tx = xtrain.T\n",
    "#         ty = ytrain.T\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.init_Params()\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            \n",
    "            self.forePropagate(tx)\n",
    "            \n",
    "            self.cost_calc(ty,loss_function = loss_function)\n",
    "            \n",
    "            self.back_prop(ty)\n",
    "            \n",
    "            self.param_update()\n",
    "            \n",
    "            self.lr_change.append(learning_rate)\n",
    "            \n",
    "#             if lr_decay and i % lr_decay_epoch == 0 and decay_stop > 0:\n",
    "#                 self.learning_rate = self.learning_rate * lr_decay_rate\n",
    "#                 self.lr_change.append(self.learning_rate)\n",
    "#                 decay_stop -= 1\n",
    "                \n",
    "            if verbose and i % verbose == 0 and str( self.costs[-1] ) != 'nan':\n",
    "                \n",
    "                print(f'epoch {i} : \\t cost = {self.costs[-1]}')\n",
    "                print(f'learning rate / alpha \\t{self.learning_rate}\\n')\n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "    \n",
    "    def predict(self,xtest):\n",
    "        \n",
    "        # we fore prop at first and return the last A\n",
    "        self.forePropagate(xtest)\n",
    "        \n",
    "        return np.squeeze(self.A['A'+str(len(self.total_layers)-1)])\n",
    "    \n",
    "    def mse_model_eval(self,ytest,ypreds):\n",
    "        \n",
    "        # compute the Mean Squared Error with y-preds and y-test\n",
    "        try:\n",
    "            loss = ( 1 / ( 2 * ypreds.shape[1])) * ( np.sum(np.square (  ypreds - self.A[ 'A'+str(len(self.total_layers)-1)])))\n",
    "            return np.squeeze(loss)\n",
    "        except:\n",
    "            print('Please check the indices and re-try')\n",
    "    \n",
    "    def plot_cost_to_epoch(self):\n",
    "#         self.costs = [self.costs[x] for x in range(1,len(self.costs)) if str(self.costs[x]) != 'nan' ]\n",
    "        plt.plot(self.costs, color = 'r')\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self):\n",
    "        pass\n",
    "    \n",
    "    def use_model(self,path):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ann()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_hl([3,2],activations = ['relu','relu','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13234)\n",
    "tx = np.random.random((3,100))\n",
    "ty = np.random.random((1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network structure update :[3, 3, 2, 1]\n",
      " feature(s) : 3 \n",
      " label(s) : 1 \n",
      " hidden layers : [3, 2]\n"
     ]
    }
   ],
   "source": [
    "model.register_training_data(np.random.random((3,100)),np.random.random((1,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.01298571, -0.00092539,  0.00070074],\n",
      "       [ 0.01855052,  0.0137026 , -0.0018258 ],\n",
      "       [-0.01170023,  0.01027954, -0.00834468]]), 'W2': array([[-0.01505343,  0.00870126,  0.01223903],\n",
      "       [-0.02231902,  0.00028256,  0.00310356]]), 'W3': array([[0.00538658, 0.01067838]])}\n",
      "\n",
      "{'b1': array([[ 0.18779336],\n",
      "       [-0.64842091],\n",
      "       [ 0.82941762]]), 'b2': array([[0.55446558],\n",
      "       [0.29317822]]), 'b3': array([[-1.73308753]])}\n"
     ]
    }
   ],
   "source": [
    "model.forePropagate(tx)\n",
    "\n",
    "print(model.W)\n",
    "print()\n",
    "print(model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cost_calc(ty,loss_function = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.back_prop(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dW3': array([[-0.61511431, -0.31935751]]), 'dW2': array([[-0.00167989,  0.        , -0.00762725],\n",
      "       [-0.00370527,  0.        , -0.01682312]]), 'dW1': array([[ 5.23612320e-05,  4.75908018e-05,  5.90970276e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [-7.11381417e-05, -6.43758711e-05, -7.94984026e-05]])}\n"
     ]
    }
   ],
   "source": [
    "print(model.dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.param_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : \t cost = 19.000877010743412\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 25 : \t cost = 9.159337270682158\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 50 : \t cost = 3.555939741549811\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 75 : \t cost = 0.6570108375213615\n",
      "learning rate / alpha \t0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-195-c0d2be9b5c1d>:130: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.dA['dA'+str(len(self.total_layers)-1)] = -1 * np.divide(Y,self.A['A'+str(len(self.total_layers)-1)]) + np.divide(1-Y, 1-self.A['A'+str(len(self.total_layers)-1)])\n",
      "<ipython-input-195-c0d2be9b5c1d>:161: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , relu(self.Z['Z'+str(i)]) )\n"
     ]
    }
   ],
   "source": [
    "model.fit(tx,ty,verbose=25,lr_decay=True,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.plot_cost_to_epoch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20e245c5700>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.lr_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
