{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pydANN ( python deep Artificial Neural Network )\n",
    "\n",
    "is a free and open source python library to implement the Machine Learning algorithm of neural networks\n",
    "The network can be as simple as a sinle layer perceptron net or a multi-layer deep neural net.\n",
    "THe design and modifications of this library is posted [here](https://www.github.com/ShimronAlakkal)\n",
    "\n",
    "\n",
    "### 1 - Packages\n",
    "\n",
    "These are some of the most important packages that you're going to need in order to use ***```pydANN```***\n",
    "\n",
    "- [numpy](www.numpy.org) (or numeric python) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs -- which are optional -- in Python.\n",
    "- [pickle](https://docs.python.org/3/library/pickle.html) is the library pydANN uses to save your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tools (functions) that are called inside of the activation layer.\n",
    "To specify the activation function for a layer use `activation_specific = f` with `addHL()`,  where `f` is a list, of length of hidden layers + 1, and each index with a custom function name.\n",
    "If there is a mismatch in the input activation_specifics, the model is going to auto adjust the activation with the last ones from your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / 1 + np.exp(-Z)\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    return np.maximum(0.1*Z)\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * ( 1 - sigmoid(Z) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden_Layers = [3,2]\n",
    "        self.total_layers = []\n",
    "        self.learning_rate = 0.001\n",
    "        self.epoch = 0\n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        self.Z = {}\n",
    "        self.A = {}\n",
    "        self.activation_functions = []\n",
    "        self.costs = [0]\n",
    "        self.dW = {}\n",
    "        self.db = {}\n",
    "        self.dZ = {}\n",
    "        self.dA = {}\n",
    "        self.lr_change = []\n",
    "        \n",
    "    \n",
    "    \n",
    "    def add_hl(self,hl,activations ):\n",
    "        self.hidden_Layers.clear()\n",
    "        self.hidden_Layers = hl\n",
    "        \n",
    "        # settingf the activations\n",
    "        if len(activations) == len(hl)+1:\n",
    "            self.activation_functions = activations\n",
    "        else:\n",
    "            print('Passed activations should be 1 more than the HL length \\n recall the function to override HL')\n",
    "                \n",
    "        \n",
    "    def clear_instance_data():\n",
    "        self.total_layers.clear()\n",
    "        self.hidden_Layers.clear()\n",
    "        self.Z.clear()\n",
    "        self.W.clear()\n",
    "        self.b.clear()\n",
    "            \n",
    "            \n",
    "    def register_training_data(self,train_x,train_y):\n",
    "        self.total_layers.clear()\n",
    "        self.total_layers.append(train_x.shape[0])\n",
    "        for i in self.hidden_Layers:\n",
    "            self.total_layers.append(i)\n",
    "       \n",
    "        # network structure\n",
    "        self.total_layers.append(train_y.shape[0])\n",
    "        print(f\"Network structure update :{self.total_layers}\\n feature(s) : {self.total_layers[0]} \\n label(s) : {self.total_layers[-1]} \\n hidden layers : {self.hidden_Layers}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_Params(self,verbose = False):\n",
    "        \n",
    "        # creating the weights and biases with seed(1)\n",
    "        np.random.seed(144)\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            \n",
    "            self.W['W'+str(i)] = np.random.randn(self.total_layers[i],self.total_layers[i-1]) * 0.01\n",
    "            self.b['b'+str(i)] = np.random.randn(self.total_layers[i],1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('shape of weight(s) initialized : \\n ')\n",
    "            for i in self.W.values():\n",
    "                print(i.shape)\n",
    "            print('shape of bias(es) initialized : \\n ')\n",
    "            for i in self.b.values():\n",
    "                print(i.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forePropagate(self,train_x):\n",
    "        self.A['A0'] = train_x\n",
    "        a = self.activation_functions\n",
    "        \n",
    "        # populating Z and A with data\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            \n",
    "            # the formula for fore-propagation is z = W.X + b\n",
    "            self.Z[ 'Z'+str(i) ] = np.dot( self.W['W'+str(i)] , self.A['A'+str(i-1)] ) + self.b['b'+str(i)]\n",
    "          \n",
    "            # populating the activation dictionary with index values\n",
    "            \n",
    "            if a[i-1] == 'relu':\n",
    "                self.A['A'+str(i)] = relu( self.Z['Z'+str(i)] )\n",
    "            elif a[i-1] == 'sigmoid': \n",
    "                self.A['A'+str(i)] = sigmoid( self.Z['Z'+str(i)] )\n",
    "            elif a[i-1] == 'sigmoid_d':\n",
    "                self.A['A'+str(i)] = sigmoid_derivative( self.Z['Z'+str(i)] )\n",
    "            else:\n",
    "                self.A['A'+str(i)] = leaky_relu( self.Z['Z'+str(i)] )\n",
    "                \n",
    "    \n",
    "    def cost_calc(self,Y,loss_function = 'mse'):\n",
    "        \n",
    "        # the `m` used in cost functions represent the total number of training examples\n",
    "        if loss_function in ['mse','MSE']:\n",
    "            \n",
    "            # use mean squared error function     \n",
    "            loss = ( 1 / ( 2 * Y.shape[1])) * ( np.sum(np.square (  Y - self.A[ 'A'+str(len(self.total_layers)-1)])))\n",
    "            cost = np.squeeze(loss)\n",
    "         \n",
    "            self.costs.append(cost)\n",
    "            \n",
    "            \n",
    "        else : #['rmse','RMSE']:\n",
    "            \n",
    "            # use the root mean squared function\n",
    "            loss = np.sqrt( ( 1 / Y.shape[1]) * ( np.sum(np.square (  Y - self.A[ 'A'+str(len(self.total_layers)-1)])))) \n",
    "            cost = np.squeeze(loss)\n",
    "            \n",
    "            self.costs.append(cost)\n",
    "        \n",
    "#         elif loss_function in ['mae','MAE']:\n",
    "            \n",
    "#             use the mean absolute error function here \n",
    "#             self.costs.append( 1 / Y.shape[1] * (np.sum(  )) )  # you're going to have to do modulus here\n",
    "            \n",
    "#         else:\n",
    "            \n",
    "#             # use binary cross entropy\n",
    "#             self.costs.append( np.squeeze(-1 * np.sum( np.multiply( Y ,np.log(self.A[ 'A'+str(len(self.total_layers)-1)]) ) +\n",
    "#                                                         np.multiply( (1-Y),np.log(1-self.A[ 'A'+str(len(self.total_layers)-1)]) ) ) / Y.shape[1] ) )\n",
    "            \n",
    "            \n",
    "            \n",
    "    def back_prop(self,Y):\n",
    "        \n",
    "        # compute dA final layer \n",
    "        self.dA['dA'+str(len(self.total_layers)-1)] = -1 * np.divide(Y,self.A['A'+str(len(self.total_layers)-1)]) + np.divide(1-Y, 1-self.A['A'+str(len(self.total_layers)-1)])\n",
    "        \n",
    "        \n",
    "        # check for the final layer activation_func\n",
    "        if self.activation_functions[-1] == 'sigmoid':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , sigmoid(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        elif self.activation_functions[-1] == 'sigmoid_d':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , sigmoid_derivative(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        elif self.activation_functions[-1] == 'relu':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , relu(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        else:\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , leaky_relu(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        \n",
    "        # get dW final layer\n",
    "        self.dW['dW'+str(len(self.total_layers)-1)] = ( 1 / Y.shape[1] ) * np.dot( self.dZ['dZ'+str(len(self.total_layers)-1)] , self.A['A'+str(len(self.total_layers)-2)].T )\n",
    "        \n",
    "        # get db final layer \n",
    "        self.db['db'+str(len(self.total_layers)-1)] = (1/Y.shape[1]) * np.sum(self.dZ['dZ'+str(len(self.total_layers)-1)],axis = 1, keepdims = True)\n",
    "        \n",
    "        self.dA['dA'+str(len(self.total_layers)-2)] = np.dot(self.W['W'+str(len(self.total_layers)-1)].T , self.dZ['dZ'+str(len(self.total_layers)-1)] )\n",
    "        \n",
    "        \n",
    "        # loop over the number of hidden layers + 1 in the network in reverse and find weights and biases for them\n",
    "        for i in reversed(range(1,len(self.total_layers)-1)):\n",
    "            \n",
    "            # check for DZ and get it done\n",
    "            if self.activation_functions[i] == 'sigmoid':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , sigmoid(self.Z['Z'+str(i)]) )\n",
    "            elif self.activation_functions[i] == 'sigmoid_d':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , sigmoid_derivative(self.Z['Z'+str(i)]) )\n",
    "            elif self.activation_functions[i] == 'relu':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , relu(self.Z['Z'+str(i)]) )\n",
    "            else:\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , leaky_relu(self.Z['Z'+str(i)]) )\n",
    "      \n",
    "            \n",
    "            self.dW['dW'+str(i)] = np.dot(self.dZ['dZ'+str(i)], self.A['A'+str(i - 1)].T) / Y.shape[1]\n",
    "            self.db['db'+str(i)] = np.sum(self.dZ['dZ'+str(i)], axis = 1, keepdims = True) / Y.shape[1]\n",
    "            self.dA['dA'+str(i - 1)] = np.dot(self.W['W'+str(i)].T, self.dZ['dZ'+str(i)])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def param_update(self):\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            self.W['W'+str(i)] -= self.learning_rate * self.dW['dW'+str(i)]\n",
    "            self.b['b'+str(i)] -= self.learning_rate * self.db['db'+str(i)]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def fit(self,xtrain,ytrain,epoch = 5000, learning_rate = 0.01,lr_decay_rate = 0.99, verbose = 0,lr_decay = False,lr_decay_epoch = 10, loss_function = 'mse'):\n",
    "#         tx = xtrain.T\n",
    "#         ty = ytrain.T\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.init_Params()\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            \n",
    "            self.forePropagate(tx)\n",
    "            \n",
    "            self.cost_calc(ty,loss_function = loss_function)\n",
    "            \n",
    "            self.back_prop(ty)\n",
    "            \n",
    "            self.param_update()\n",
    "            \n",
    "            self.lr_change.append(learning_rate)\n",
    "            \n",
    "            if lr_decay and i % lr_decay_epoch == 0 and lr_decay_rate > 0:\n",
    "                self.learning_rate = self.learning_rate * lr_decay_rate\n",
    "                lr_decay_epoch -= 1\n",
    "                \n",
    "            if verbose and i % verbose == 0 and str( self.costs[-1] ) != 'nan':\n",
    "                \n",
    "                print(f'epoch {i} : \\t cost = {self.costs[-1]}')\n",
    "                print(f'learning rate / alpha \\t{self.learning_rate}\\n')\n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "    \n",
    "    def predict(self,xtest):\n",
    "        pass\n",
    "    \n",
    "    def mse_model_eval(self,ypreds):\n",
    "        pass\n",
    "    \n",
    "    def plot_cost_to_epoch(self,epoch):\n",
    "        self.costs = [self.costs[x] for x in range(len(self.costs)) if str(self.costs[x]) != 'nan' ]\n",
    "    \n",
    "    def save_model(self):\n",
    "        pass\n",
    "    \n",
    "    def use_model(self,path):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ann()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_hl([3,2],activations = ['relu','relu','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13234)\n",
    "tx = np.random.random((3,100))\n",
    "ty = np.random.random((1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network structure update :[3, 3, 2, 1]\n",
      " feature(s) : 3 \n",
      " label(s) : 1 \n",
      " hidden layers : [3, 2]\n"
     ]
    }
   ],
   "source": [
    "model.register_training_data(np.random.random((3,100)),np.random.random((1,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.01298571, -0.00092539,  0.00070074],\n",
      "       [ 0.01855052,  0.0137026 , -0.0018258 ],\n",
      "       [-0.01170023,  0.01027954, -0.00834468]]), 'W2': array([[-0.01505343,  0.00870126,  0.01223903],\n",
      "       [-0.02231902,  0.00028256,  0.00310356]]), 'W3': array([[0.00538658, 0.01067838]])}\n",
      "\n",
      "{'b1': array([[ 0.18779336],\n",
      "       [-0.64842091],\n",
      "       [ 0.82941762]]), 'b2': array([[0.55446558],\n",
      "       [0.29317822]]), 'b3': array([[-1.73308753]])}\n"
     ]
    }
   ],
   "source": [
    "model.forePropagate(tx)\n",
    "\n",
    "print(model.W)\n",
    "print()\n",
    "print(model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cost_calc(ty,loss_function = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.back_prop(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dW3': array([[-0.61511431, -0.31935751]]), 'dW2': array([[-0.00167989,  0.        , -0.00762725],\n",
      "       [-0.00370527,  0.        , -0.01682312]]), 'dW1': array([[ 5.23612320e-05,  4.75908018e-05,  5.90970276e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [-7.11381417e-05, -6.43758711e-05, -7.94984026e-05]])}\n"
     ]
    }
   ],
   "source": [
    "print(model.dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.param_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : \t cost = 19.000877010743412\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 1 : \t cost = 18.47738164014651\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 2 : \t cost = 17.967981615778015\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 3 : \t cost = 17.47212247260455\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 4 : \t cost = 16.989278447102347\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 5 : \t cost = 16.51895084630715\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 6 : \t cost = 16.060666526202006\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 7 : \t cost = 15.61397647138884\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 8 : \t cost = 15.178454468635511\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 9 : \t cost = 14.75369586747824\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 10 : \t cost = 14.339316421595209\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 11 : \t cost = 13.934951205155786\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 12 : \t cost = 13.540253598795655\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 13 : \t cost = 13.154894340275044\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 14 : \t cost = 12.778560635249002\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 15 : \t cost = 12.41095532391847\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 16 : \t cost = 12.051796099641175\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 17 : \t cost = 11.700814775865956\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 18 : \t cost = 11.357756598013738\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 19 : \t cost = 11.022379597166788\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 20 : \t cost = 10.694453982645786\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 21 : \t cost = 10.37376157075439\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 22 : \t cost = 10.060095247153948\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 23 : \t cost = 9.753258460499413\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 24 : \t cost = 9.453064745121527\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 25 : \t cost = 9.159337270682158\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 26 : \t cost = 8.871908416859613\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 27 : \t cost = 8.590619371240306\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 28 : \t cost = 8.315319748703057\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 29 : \t cost = 8.045867230683369\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 30 : \t cost = 7.7821272227981115\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 31 : \t cost = 7.523972529396708\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 32 : \t cost = 7.271283043684059\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 33 : \t cost = 7.02394545213337\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 34 : \t cost = 6.781852951974461\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 35 : \t cost = 6.544904980605654\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 36 : \t cost = 6.313006955835175\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 37 : \t cost = 6.0860700259117735\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 38 : \t cost = 5.864010828354451\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 39 : \t cost = 5.646751256637912\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 40 : \t cost = 5.434218233834228\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 41 : \t cost = 5.226343492352433\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 42 : \t cost = 5.023063358956706\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 43 : \t cost = 4.824318544280664\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 44 : \t cost = 4.630053936090454\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 45 : \t cost = 4.440218395582995\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 46 : \t cost = 4.254764556037989\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 47 : \t cost = 4.073648623173654\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 48 : \t cost = 3.8968301765863345\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 49 : \t cost = 3.7242719716836765\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 50 : \t cost = 3.555939741549811\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 51 : \t cost = 3.391801998209005\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 52 : \t cost = 3.2318298327815667\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 53 : \t cost = 3.0759967140522733\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 54 : \t cost = 2.924278284996976\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 55 : \t cost = 2.7766521568370233\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 56 : \t cost = 2.6330977002132245\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 57 : \t cost = 2.4935958330904286\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 58 : \t cost = 2.35812880501954\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 59 : \t cost = 2.2266799773944026\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 60 : \t cost = 2.0992335993447875\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 61 : \t cost = 1.975774578901151\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 62 : \t cost = 1.8562882490489419\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 63 : \t cost = 1.7407601282559901\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 64 : \t cost = 1.6291756750012494\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 65 : \t cost = 1.521520035751436\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 66 : \t cost = 1.4177777857185954\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 67 : \t cost = 1.3179326615831846\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 68 : \t cost = 1.221967285187588\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 69 : \t cost = 1.129862877016079\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 70 : \t cost = 1.041598958144907\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 71 : \t cost = 0.9571530394380487\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 72 : \t cost = 0.8765002974846766\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 73 : \t cost = 0.7996132390827881\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 74 : \t cost = 0.7264613622624926\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 75 : \t cost = 0.6570108375213615\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 76 : \t cost = 0.5912242723546016\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 77 : \t cost = 0.5290607233407123\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 78 : \t cost = 0.4704763914545495\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 79 : \t cost = 0.41542721415252215\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 80 : \t cost = 0.3638770114588264\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 81 : \t cost = 0.3158235403952389\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 82 : \t cost = 0.27139153652909115\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 83 : \t cost = 0.23123839577867278\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 84 : \t cost = 0.19892489680877248\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 85 : \t cost = 0.1891801871792373\n",
      "learning rate / alpha \t0.01\n",
      "\n",
      "epoch 86 : \t cost = 0.1891801579091628\n",
      "learning rate / alpha \t0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-411-dba37d5129d6>:130: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.dA['dA'+str(len(self.total_layers)-1)] = -1 * np.divide(Y,self.A['A'+str(len(self.total_layers)-1)]) + np.divide(1-Y, 1-self.A['A'+str(len(self.total_layers)-1)])\n",
      "<ipython-input-411-dba37d5129d6>:161: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , relu(self.Z['Z'+str(i)]) )\n"
     ]
    }
   ],
   "source": [
    "model.fit(tx,ty,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.plot_cost_to_epoch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
