{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pydANN ( python deep Artificial Neural Network )\n",
    "\n",
    "is a free and open source python library to implement the Machine Learning algorithm of neural networks\n",
    "The network can be as simple as a sinle layer perceptron net or a multi-layer deep neural net.\n",
    "THe design and modifications of this library is posted [here](https://www.github.com/ShimronAlakkal)\n",
    "\n",
    "\n",
    "### 1 - Packages\n",
    "\n",
    "These are some of the most important packages that you're going to need in order to use ***```pydANN```***\n",
    "\n",
    "- [numpy](www.numpy.org) (or numeric python) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs -- which are optional -- in Python.\n",
    "- [pickle](https://docs.python.org/3/library/pickle.html) is the library pydANN uses to save your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tools (functions) that are called inside of the activation layer.\n",
    "To specify the activation function for a layer use `activation_specific = f` with `addHL()`,  where `f` is a list, of length of hidden layers + 1, and each index with a custom function name.\n",
    "If there is a mismatch in the input activation_specifics, the model is going to auto adjust the activation with the last ones from your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-1 * Z))\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    return np.maximum(0.1*Z)\n",
    "\n",
    "def sigmoid_d(Z):\n",
    "    return sigmoid(Z) * ( 1 - sigmoid(Z) ) \n",
    "\n",
    "def relu_d(Z):\n",
    "    Z[Z >= 0] = 1\n",
    "    Z[Z < 0]  = 0\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden_Layers = [3,2]\n",
    "        self.total_layers = []\n",
    "        self.learning_rate = 0\n",
    "        self.epoch = 0\n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        self.Z = {}\n",
    "        self.A = {}\n",
    "        self.activation_functions = []\n",
    "        self.costs = [0]\n",
    "        self.dW = {}\n",
    "        self.db = {}\n",
    "        self.dZ = {}\n",
    "        self.dA = {}\n",
    "        self.lr_change = []\n",
    "        \n",
    "    \n",
    "    \n",
    "    def add_hl(self,hl,activations ):\n",
    "        self.hidden_Layers.clear()\n",
    "        self.hidden_Layers = hl\n",
    "        \n",
    "        # settingf the activations\n",
    "        if len(activations) == len(hl)+1:\n",
    "            self.activation_functions = activations\n",
    "        else:\n",
    "            print('Passed activations should be 1 more than the HL length \\n recall the function to override HL')\n",
    "                \n",
    "        \n",
    "    def dispose_model(self):\n",
    "        self.total_layers.clear()\n",
    "        self.hidden_Layers.clear()\n",
    "        self.costs.clear()\n",
    "        self.lr_change.clear()\n",
    "        self.Z.clear()\n",
    "        self.W.clear()\n",
    "        self.b.clear()\n",
    "        self.db.clear()\n",
    "        self.dW.clear()\n",
    "        self.dZ.clear()\n",
    "        self.dA.clear()\n",
    "            \n",
    "    def register_training_data(self,train_x,train_y):\n",
    "        self.total_layers.clear()\n",
    "        self.total_layers.append(train_x.shape[0])\n",
    "        for i in self.hidden_Layers:\n",
    "            self.total_layers.append(i)\n",
    "       \n",
    "        # network structure\n",
    "        self.total_layers.append(train_y.shape[0])\n",
    "        print(f\"Network structure update :{self.total_layers}\\n feature(s) : {self.total_layers[0]} \\n label(s) : {self.total_layers[-1]} \\n hidden layers : {self.hidden_Layers}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_Params(self,verbose = False):\n",
    "        \n",
    "        # creating the weights and biases with seed(1)\n",
    "        np.random.seed(144)\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            \n",
    "            self.W['W'+str(i)] = np.random.randn(self.total_layers[i],self.total_layers[i-1]) * 0.01\n",
    "            self.b['b'+str(i)] = np.random.randn(self.total_layers[i],1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('shape of weight(s) initialized : \\n ')\n",
    "            for i in self.W.values():\n",
    "                print(i.shape)\n",
    "            print('shape of bias(es) initialized : \\n ')\n",
    "            for i in self.b.values():\n",
    "                print(i.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forePropagate(self,train_x):\n",
    "        self.A['A0'] = train_x\n",
    "        a = self.activation_functions\n",
    "        \n",
    "        # populating Z and A with data\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            \n",
    "            # the formula for fore-propagation is z = W.X + b\n",
    "            self.Z[ 'Z'+str(i) ] = np.dot( self.W['W'+str(i)] , self.A['A'+str(i-1)] ) + self.b['b'+str(i)]\n",
    "          \n",
    "            # populating the activation dictionary with index values\n",
    "            \n",
    "            if a[i-1] == 'relu':\n",
    "                self.A['A'+str(i)] = relu( self.Z['Z'+str(i)] )\n",
    "            elif a[i-1] == 'sigmoid': \n",
    "                self.A['A'+str(i)] = sigmoid( self.Z['Z'+str(i)] )\n",
    "            else:\n",
    "                self.A['A'+str(i)] = leaky_relu( self.Z['Z'+str(i)] )\n",
    "                \n",
    "    \n",
    "    def cost_calc(self,Y,loss_function ):\n",
    "        \n",
    "        # the `m` used in cost functions represent the total number of training examples\n",
    "        if loss_function in ['mse','MSE']:\n",
    "            \n",
    "            # use mean squared error function     \n",
    "            loss = ( 1 /  Y.shape[1]) * ( np.sum(np.square (  Y - self.A[ 'A'+str(len(self.total_layers)-1)])))\n",
    "            cost = np.squeeze(loss)\n",
    "         \n",
    "            self.costs.append(cost)\n",
    "            \n",
    "            \n",
    "        else : #['rmse','RMSE']:\n",
    "            \n",
    "            # use the root mean squared function\n",
    "            loss = np.sqrt( ( 1 / Y.shape[1]) * ( np.sum(np.square (  Y - self.A[ 'A'+str(len(self.total_layers)-1)])))) \n",
    "            cost = np.squeeze(loss)\n",
    "            \n",
    "            self.costs.append(cost)\n",
    "        \n",
    "#         elif loss_function in ['mae','MAE']:\n",
    "            \n",
    "#             use the mean absolute error function here \n",
    "#             self.costs.append( 1 / Y.shape[1] * (np.sum(  )) )  # you're going to have to do modulus here\n",
    "            \n",
    "#         else:\n",
    "            \n",
    "#             # use binary cross entropy\n",
    "#             self.costs.append( np.squeeze(-1 * np.sum( np.multiply( Y ,np.log(self.A[ 'A'+str(len(self.total_layers)-1)]) ) +\n",
    "#                                                         np.multiply( (1-Y),np.log(1-self.A[ 'A'+str(len(self.total_layers)-1)]) ) ) / Y.shape[1] ) )\n",
    "            \n",
    "            \n",
    "            \n",
    "    def back_prop(self,Y):\n",
    "        \n",
    "        # compute dA final layer \n",
    "        self.dA['dA'+str(len(self.total_layers)-1)] =  (-1 * np.divide(Y,self.A['A'+str(len(self.total_layers)-1)])) - np.divide(1-Y, 1-self.A['A'+str(len(self.total_layers)-1)])\n",
    "        \n",
    "        \n",
    "        # check for the final layer activation_func\n",
    "        if self.activation_functions[-1] == 'sigmoid':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , sigmoid_d(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        elif self.activation_functions[-1] == 'relu':\n",
    "            self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , relu_d(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "#         else:\n",
    "#             self.dZ['dZ'+str(len(self.total_layers)-1)] = np.multiply( self.dA['dA'+str(len(self.total_layers)-1)] , leaky_relu(self.Z['Z'+str(len(self.total_layers)-1)]) )\n",
    "        \n",
    "        # get dW final layer\n",
    "        self.dW['dW'+str(len(self.total_layers)-1)] = ( 1 / Y.shape[1] ) * np.dot( self.dZ['dZ'+str(len(self.total_layers)-1)] , self.A['A'+str(len(self.total_layers)-2)].T )\n",
    "        \n",
    "        # get db final layer \n",
    "        self.db['db'+str(len(self.total_layers)-1)] = (1/Y.shape[1]) * np.sum(self.dZ['dZ'+str(len(self.total_layers)-1)],axis = 1, keepdims = True)\n",
    "        \n",
    "        self.dA['dA'+str(len(self.total_layers)-2)] = np.dot(self.W['W'+str(len(self.total_layers)-1)].T , self.dZ['dZ'+str(len(self.total_layers)-1)] )\n",
    "        \n",
    "        \n",
    "        # loop over the number of hidden layers + 1 in the network in reverse and find weights and biases for them\n",
    "        for i in reversed(range(1,len(self.total_layers)-1)):\n",
    "            \n",
    "            # check for DZ and get it done\n",
    "            if self.activation_functions[i] == 'sigmoid':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , sigmoid_d(self.Z['Z'+str(i)]) )\n",
    "            elif self.activation_functions[i] == 'relu':\n",
    "                self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , relu_d(self.Z['Z'+str(i)]) )\n",
    "#             else:\n",
    "#                 self.dZ['dZ'+str(i)] = np.multiply( self.dA['dA'+str(i)] , leaky_relu_d(self.Z['Z'+str(i)]) )\n",
    "      \n",
    "            \n",
    "            self.dW['dW'+str(i)] = np.dot(self.dZ['dZ'+str(i)], self.A['A'+str(i - 1)].T) / Y.shape[1]\n",
    "            self.db['db'+str(i)] = np.sum(self.dZ['dZ'+str(i)], axis = 1, keepdims = True) / Y.shape[1]\n",
    "            self.dA['dA'+str(i - 1)] = np.dot(self.W['W'+str(i)].T, self.dZ['dZ'+str(i)])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def param_update(self):\n",
    "        for i in range(1,len(self.total_layers)):\n",
    "            self.W['W'+str(i)] -= self.learning_rate * self.dW['dW'+str(i)]\n",
    "            self.b['b'+str(i)] -= self.learning_rate * self.db['db'+str(i)]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def fit(self,xtrain,ytrain,epoch = 50, learning_rate = 0.01, verbose = 0,decay = True, decay_iter = 5, decay_rate = 0.9, stop_decay_counter = 100, loss_function = 'mse'):\n",
    "#         tx = xtrain.T\n",
    "#         ty = ytrain.T\n",
    "        self.learning_rate = learning_rate\n",
    "        self.costs.clear()\n",
    "        self.lr_change.clear()\n",
    "        \n",
    "        self.register_training_data(xtrain,ytrain)\n",
    "        self.init_Params()\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            \n",
    "            self.forePropagate(tx)\n",
    "            \n",
    "            self.cost_calc(ty,loss_function = loss_function)\n",
    "            \n",
    "            self.back_prop(ty)\n",
    "            \n",
    "            self.param_update()\n",
    "            \n",
    "            self.lr_change.append(self.learning_rate)\n",
    "            \n",
    "            if decay and stop_decay_counter > 0 and i % decay_iter == 0:\n",
    "                self.learning_rate = decay_rate * self.learning_rate\n",
    "                stop_decay_counter -= 1\n",
    "                self.lr_change.append(self.learning_rate)\n",
    "                \n",
    "            if verbose and i % verbose == 0 and str( self.costs[-1] ) != 'nan':\n",
    "                \n",
    "                print(f'epoch {i} : \\t cost = {self.costs[-1]}')\n",
    "                print(f'learning rate / alpha \\t{self.learning_rate}\\n')\n",
    "            \n",
    "        print(f'epoch {epoch}:\\t  cost = {self.costs[-1]}')\n",
    "        print(f'learning rate / alpha \\t{self.learning_rate}\\n')\n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "    \n",
    "    def predict(self,xtest):\n",
    "        \n",
    "        # we fore prop at first and return the last A\n",
    "        self.forePropagate(xtest)\n",
    "        \n",
    "        return self.A['A'+str(len(self.total_layers)-1)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def mse_model_eval(self,ytest,ypreds):\n",
    "        \n",
    "        # compute the Mean Squared Error with y-preds and y-test\n",
    "        try:\n",
    "            loss = ( 1 / ypreds.shape[1]) * ( np.sum(np.square (  ypreds - self.A[ 'A'+str(len(self.total_layers)-1)])))\n",
    "            print(np.squeeze(loss))\n",
    "        except:\n",
    "            print('Please check the indices and re-try')\n",
    "            \n",
    "            \n",
    "    \n",
    "    def plot_cost_to_epoch(self):\n",
    "        self.costs = [self.costs[x] for x in range(1,len(self.costs)) if str(self.costs[x]) != 'nan' ]\n",
    "        plt.plot(self.costs, color = 'r')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_lrc_to_epoch(self):\n",
    "        plt.plot(self.lr_change)\n",
    "        plt.show;\n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_model(self,file = 'pydann_model.dat'):\n",
    "        \n",
    "        model = {'w':self.W,'b':self.b,'lr':self.learning_rate,'actvns':self.activation_functions,\n",
    "                        'hl':self.hidden_Layers,'tl':self.total_layers}\n",
    "        with open(file,'wb') as file :\n",
    "            pickle.dump(model,file)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def use_model(self,path):\n",
    "        try:\n",
    "            with open(path,'rb') as file:\n",
    "                model = pickle.load(file)\n",
    "                self.W = model['w']\n",
    "                self.b = model['b']\n",
    "                self.activation_functions = model['actvns']\n",
    "                self.total_layers = model['tl']\n",
    "                self.hidden_Layers = model['hl']\n",
    "                self.learning_rate = model['lr']\n",
    "        except:\n",
    "            print(f\"unable to open {path}\\n check if you've added the file extension(.dat) with the file path\")\n",
    "    \n",
    "    \n",
    "    def auto_model_setup(self,seed):\n",
    "        pass\n",
    "    \n",
    "    def help(self):\n",
    "        print('''\n",
    "        \\033[1m \n",
    "        ANN model Help\n",
    "        \n",
    "        Inorder to make an Artificial Neural Network model using pydANN, intanciate the class ann()\n",
    "        \\033[0m \n",
    "        model = ann()\n",
    "        \n",
    "        \\033[1m\n",
    "        Inorder to add hidden layers to the model use the method add_hl( hidden_Layers_list , activations_for_each_layer + 1 )\n",
    "        \\033[0m\n",
    "        \n",
    "        \n",
    "        # we are now going to add hidden layers to the model \n",
    "\n",
    "        model.add_hl( [ 2,5,5 ] , activation_functions = ['relu', 'relu' ,'relu' ,'sigmoid'] )\n",
    "\n",
    "        # the above line adds to the model three hidden layers each of 2, 5 and 5 nodes, respectively. \n",
    "        # The activation_functions should be 1 more in length so as to have one for each hl and an activation for the output layer\n",
    "        \n",
    "        \\033[1m\n",
    "        To train the model, use the fit( xtrain, ytrain, epoch = 50, learning_rate = 0.01, verbose = 0,decay = True, decay_iter = 5, decay_rate = 0.9, stop_decay_counter = 100, loss_function = 'mse' ) method\n",
    "        \\033[0m\n",
    "        \n",
    "        # you can also register the training data before you train the model.\n",
    "\n",
    "        model.fit( train_x , y_train )  # this is the basic implementation of the method without any alteration\n",
    "        epoch : The number of iterations to train the model\n",
    "         \n",
    "         \n",
    "        \\033[1m \n",
    "        learning_rate : \\033[0m The value with which the algorithm optimizes weights and biases\n",
    "\\033[1m\n",
    "        verbose :\\033[0m When put 0, verbose is False ( There won't be data printing ) if set to another value other than 0, then the data is printed after each verbose interval\n",
    "\\033[1m\n",
    "        decay :\\033[0m This value optimizes the learning_rate when set to True\n",
    "\\033[1m\n",
    "        loss_function :\\033[0m The function using which the loss is calculated. Loss function option : 'mse' , 'rmse'\n",
    "\\033[1m\n",
    "        decay_rate :\\033[0m The fractional value with which the learning rate is altered\n",
    "\\033[1m\n",
    "        To plot the change in the cost(s) after each epoch, use plot_cost_to_epoch()\n",
    "               \\033[0m\n",
    "\n",
    "        model.plot_cost_to_epoch()\n",
    "        \n",
    "        \\033[1m\n",
    "        To plot the change in learning_rate after each epoch, use plot_lrc_to_epoch()\n",
    "        model.plot_lrc_to_epoch()\n",
    "        \n",
    "                \\033[0m\n",
    "\n",
    "        \\033[1m \n",
    "        To predict on test data, use the function predict( xtest )\n",
    "        \\033[0m\n",
    "        y_predictions = model.predict( xtest )\n",
    "        \n",
    "        \\033[1m\n",
    "        To evaluate the model base on Mean Squared Error, use mse_model_eval( ytest,ypreds )\n",
    "        \\033[0m\n",
    "        model.mse_model_eval( ytest,ypreds )\n",
    "        \n",
    "        \\033[1m\n",
    "        To save the current trained model use method save_model( file = 'pydann_model.dat' )\n",
    "        \\033[0m\n",
    "        \n",
    "        # 'file' is the name of the file in which the model would be saved and it has to be a .dat file\n",
    "        model.save_model()     # to save the model as pydann_model.dat just call the function like this\n",
    "\n",
    "        # to customize the name of the file in which the model should be saved , change 'file'\n",
    "        model.save_model( file = 'model.dat' )\n",
    "       \n",
    "       \\033[1m\n",
    "       To use the saved model use method use_model( path )\n",
    "        \\033[0m\n",
    "        # path is the file path ( directory ) to the saved model\n",
    "        model.use_model( )\n",
    "        \n",
    "        \\033[1m\n",
    "        To dispose the model and clear memory use the dispose_model() method\n",
    "        \\033[0m\n",
    "        model.dispose_model() \n",
    "        \n",
    "        ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the model on meaningless data which might make the whole thing go wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ann()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \u001b[1m \n",
      "        ANN model Help\n",
      "        \n",
      "        Inorder to make an Artificial Neural Network model using pydANN, intanciate the class ann()\n",
      "        \u001b[0m \n",
      "        model = ann()\n",
      "        \n",
      "        \u001b[1m\n",
      "        Inorder to add hidden layers to the model use the method add_hl( hidden_Layers_list , activations_for_each_layer + 1 )\n",
      "        \u001b[0m\n",
      "        \n",
      "        \n",
      "        # we are now going to add hidden layers to the model \n",
      "\n",
      "        model.add_hl( [ 2,5,5 ] , activation_functions = ['relu', 'relu' ,'relu' ,'sigmoid'] )\n",
      "\n",
      "        # the above line adds to the model three hidden layers each of 2, 5 and 5 nodes, respectively. \n",
      "        # The activation_functions should be 1 more in length so as to have one for each hl and an activation for the output layer\n",
      "        \n",
      "        \u001b[1m\n",
      "        To train the model, use the fit( xtrain, ytrain, epoch = 50, learning_rate = 0.01, verbose = 0,decay = True, decay_iter = 5, decay_rate = 0.9, stop_decay_counter = 100, loss_function = 'mse' ) method\n",
      "        \u001b[0m\n",
      "        \n",
      "        # you can also register the training data before you train the model.\n",
      "\n",
      "        model.fit( train_x , y_train )  # this is the basic implementation of the method without any alteration\n",
      "        epoch : The number of iterations to train the model\n",
      "         \n",
      "         \n",
      "        \u001b[1m \n",
      "        learning_rate : \u001b[0m The value with which the algorithm optimizes weights and biases\n",
      "\u001b[1m\n",
      "        verbose :\u001b[0m When put 0, verbose is False ( There won't be data printing ) if set to another value other than 0, then the data is printed after each verbose interval\n",
      "\u001b[1m\n",
      "        decay :\u001b[0m This value optimizes the learning_rate when set to True\n",
      "\u001b[1m\n",
      "        loss_function :\u001b[0m The function using which the loss is calculated. Loss function option : 'mse' , 'rmse'\n",
      "\u001b[1m\n",
      "        decay_rate :\u001b[0m The fractional value with which the learning rate is altered\n",
      "\u001b[1m\n",
      "        To plot the change in the cost(s) after each epoch, use plot_cost_to_epoch()\n",
      "               \u001b[0m\n",
      "\n",
      "        model.plot_cost_to_epoch()\n",
      "        \n",
      "        \u001b[1m\n",
      "        To plot the change in learning_rate after each epoch, use plot_lrc_to_epoch()\n",
      "        model.plot_lrc_to_epoch()\n",
      "        \n",
      "                \u001b[0m\n",
      "\n",
      "        \u001b[1m \n",
      "        To predict on test data, use the function predict( xtest )\n",
      "        \u001b[0m\n",
      "        y_predictions = model.predict( xtest )\n",
      "        \n",
      "        \u001b[1m\n",
      "        To evaluate the model base on Mean Squared Error, use mse_model_eval( ytest,ypreds )\n",
      "        \u001b[0m\n",
      "        model.mse_model_eval( ytest,ypreds )\n",
      "        \n",
      "        \u001b[1m\n",
      "        To save the current trained model use method save_model( file = 'pydann_model.dat' )\n",
      "        \u001b[0m\n",
      "        \n",
      "        # 'file' is the name of the file in which the model would be saved and it has to be a .dat file\n",
      "        model.save_model()     # to save the model as pydann_model.dat just call the function like this\n",
      "\n",
      "        # to customize the name of the file in which the model should be saved , change 'file'\n",
      "        model.save_model( file = 'model.dat' )\n",
      "       \n",
      "       \u001b[1m\n",
      "       To use the saved model use method use_model( path )\n",
      "        \u001b[0m\n",
      "        # path is the file path ( directory ) to the saved model\n",
      "        model.use_model( )\n",
      "        \n",
      "        \u001b[1m\n",
      "        To dispose the model and clear memory use the dispose_model() method\n",
      "        \u001b[0m\n",
      "        model.dispose_model() \n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "model.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_hl([8,6,6,3],activations = ['relu','sigmoid','relu','relu','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13234)\n",
    "tx = np.random.random((3,100))\n",
    "ty = np.random.random((1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network structure update :[3, 8, 6, 6, 3, 1]\n",
      " feature(s) : 3 \n",
      " label(s) : 1 \n",
      " hidden layers : [8, 6, 6, 3]\n"
     ]
    }
   ],
   "source": [
    "model.register_training_data(np.random.random((3,100)),np.random.random((1,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.01298571, -0.00092539,  0.00070074],\n",
      "       [ 0.01855052,  0.0137026 , -0.0018258 ],\n",
      "       [-0.01170023,  0.01027954, -0.00834468],\n",
      "       [ 0.00187793, -0.00648421,  0.00829418],\n",
      "       [-0.01505343,  0.00870126,  0.01223903],\n",
      "       [-0.02231902,  0.00028256,  0.00310356],\n",
      "       [ 0.00554466,  0.00293178,  0.00538658],\n",
      "       [ 0.01067838, -0.01733088, -0.00193664]]), 'W2': array([[ 0.00211083,  0.00659727, -0.00546553,  0.00159969, -0.00299756,\n",
      "         0.00271352,  0.01249906, -0.01398527],\n",
      "       [ 0.0023012 , -0.00224214, -0.00360311, -0.00192167,  0.01998439,\n",
      "         0.01085534, -0.01633774, -0.01309612],\n",
      "       [-0.01763754, -0.01216804, -0.00234821, -0.00437128, -0.00292036,\n",
      "         0.00276368, -0.00802196, -0.00089666],\n",
      "       [-0.00299194, -0.00312767, -0.00597397, -0.00332675,  0.00213999,\n",
      "         0.01406547,  0.00250701,  0.00974168],\n",
      "       [-0.00032676,  0.00814333, -0.00787541,  0.00264752, -0.0107492 ,\n",
      "        -0.0042068 , -0.00892893,  0.00927921],\n",
      "       [ 0.00265021,  0.01508603, -0.00075199,  0.00793893,  0.00882451,\n",
      "        -0.00211563, -0.00311329,  0.00187612]]), 'W3': array([[-0.00400013, -0.01134744, -0.01079014, -0.01456128, -0.010018  ,\n",
      "         0.00543085],\n",
      "       [ 0.00731462,  0.01034002,  0.01418882, -0.02300068,  0.00343181,\n",
      "         0.00693116],\n",
      "       [ 0.01093519,  0.00369791,  0.00107428, -0.01922627, -0.00240342,\n",
      "        -0.02004464],\n",
      "       [ 0.00503542,  0.00449346, -0.0149607 , -0.00149875,  0.01450884,\n",
      "         0.01265724],\n",
      "       [ 0.00850734,  0.01164604,  0.00904421, -0.00258371,  0.01060894,\n",
      "        -0.00439105],\n",
      "       [ 0.00441122,  0.01085463, -0.00656174,  0.00156292, -0.01027357,\n",
      "        -0.00144797]]), 'W4': array([[-0.00262879, -0.00305914, -0.00164227,  0.00788528,  0.00271657,\n",
      "         0.00334115],\n",
      "       [ 0.02246307, -0.00121998, -0.00968551, -0.02897117,  0.00577106,\n",
      "         0.00042441],\n",
      "       [ 0.00417509,  0.02080935, -0.01764104,  0.00332555, -0.00815445,\n",
      "        -0.00516989]]), 'W5': array([[ 0.0133718 , -0.00818097,  0.0152592 ]])}\n",
      "\n",
      "{'b1': array([[-0.19915519],\n",
      "       [-1.13385466],\n",
      "       [ 2.21669181],\n",
      "       [-1.17293195],\n",
      "       [ 0.55861575],\n",
      "       [-0.7900395 ],\n",
      "       [ 0.80141421],\n",
      "       [-0.03746725]]), 'b2': array([[ 0.81552061],\n",
      "       [-0.32649909],\n",
      "       [-0.12450633],\n",
      "       [-0.33937231],\n",
      "       [ 1.1005343 ],\n",
      "       [-1.71636314]]), 'b3': array([[ 1.05071634],\n",
      "       [-0.73866615],\n",
      "       [-0.56361126],\n",
      "       [ 0.91855338],\n",
      "       [-0.07438515],\n",
      "       [ 0.05651145]]), 'b4': array([[-0.89634258],\n",
      "       [-0.04324785],\n",
      "       [ 0.0351752 ]]), 'b5': array([[-0.74578468]])}\n"
     ]
    }
   ],
   "source": [
    "model.forePropagate(tx)\n",
    "\n",
    "print(model.W)\n",
    "print()\n",
    "print(model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cost_calc(ty,loss_function = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.back_prop(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dW5': array([[ 0.        ,  0.        , -0.02062989]]), 'dW4': array([[-1.38032712e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "        -1.25046763e-03,  0.00000000e+00, -7.24257207e-05],\n",
      "       [ 1.02314859e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "         9.26892015e-04,  0.00000000e+00,  5.36845742e-05],\n",
      "       [-1.90858924e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "        -1.72903149e-03,  0.00000000e+00, -1.00143618e-04]]), 'dW3': array([[ 1.25889114e-05,  7.57461018e-06,  8.46402566e-06,\n",
      "         7.51765565e-06,  1.35354197e-05,  2.77118705e-06],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [-3.16515108e-05, -1.90443676e-05, -2.12805692e-05,\n",
      "        -1.89011704e-05, -3.40312574e-05, -6.96742190e-06],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 3.84192312e-06,  2.31164309e-06,  2.58307767e-06,\n",
      "         2.29426153e-06,  4.13078148e-06,  8.45719482e-07]]), 'dW2': array([[ 0.00000000e+00,  0.00000000e+00, -6.15482471e-07,\n",
      "         0.00000000e+00, -1.56365327e-07,  0.00000000e+00,\n",
      "        -2.25042308e-07,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -1.99473079e-06,\n",
      "         0.00000000e+00, -5.06767856e-07,  0.00000000e+00,\n",
      "        -7.29344606e-07,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00]]), 'dW1': array([[-3.56666278e-11, -3.14932392e-11, -3.95705875e-11],\n",
      "       [-8.40044618e-10, -7.41701790e-10, -9.30494735e-10],\n",
      "       [ 3.78780518e-10,  3.33231041e-10,  4.20026528e-10],\n",
      "       [-2.52291949e-10, -2.22499464e-10, -2.79988680e-10],\n",
      "       [ 1.20037808e-09,  1.05856812e-09,  1.33015960e-09],\n",
      "       [ 3.19984404e-10,  2.82862787e-10,  3.55407098e-10],\n",
      "       [ 4.80435519e-10,  4.24196880e-10,  5.32922418e-10],\n",
      "       [-5.51203798e-10, -4.86561304e-10, -6.11399915e-10]])}\n"
     ]
    }
   ],
   "source": [
    "print(model.dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.param_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network structure update :[3, 8, 6, 6, 3, 1]\n",
      " feature(s) : 3 \n",
      " label(s) : 1 \n",
      " hidden layers : [8, 6, 6, 3]\n",
      "epoch 0 : \t cost = 0.11466460167592583\n",
      "learning rate / alpha \t0.00025\n",
      "\n",
      "epoch 50:\t  cost = 0.11458232791021854\n",
      "learning rate / alpha \t9.5367431640625e-10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(tx,ty,verbose=100,decay=True,decay_rate=0.25,learning_rate = 0.001,epoch=50,loss_function='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD4CAYAAAAtrdtxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAc5ElEQVR4nO3de5CcdZ3v8fdncoOIEEIGAklgokkKAxWCTKYto8jlIEHc4CkBYUHNWZestbJqeZQFy3UViy3duOdsHYra4rLuelwRgVWIRInKQc3KLZNIApOcnAwhJLNBE3KRxBRJhnzPH8/T0nR6Mj3d0/P05fOqmuru33P7PpWpfOa5fVsRgZmZWSXasi7AzMwal0PEzMwq5hAxM7OKOUTMzKxiDhEzM6vY6KwLqKVJkyZFR0dH1mWYmTWUVatWvRIR7eXM29Qh0tHRQXd3d9ZlmJk1FEkvlTuvT2eZmVnFHCJmZlYxh4iZmVXMIWJmZhVziJiZWcUcImZmVjGHiJmZVcwhUsquXXDrrbBqVdaVmJnVtaZ+2LBio0bBV76SvD/vvExLMTOrZz4SKeWEE+DMM+GZZ7KuxMysrjlEBpLLwdNPg7/50cxsQA6RgeRy8Mor8OKLWVdiZla3HCIDyeWS16efzrYOM7M65hAZyNlnwzHH+LqImdlROEQGMmZMcmeWj0TMzAbkEDmaXA5Wr4aDB7OuxMysLjlEjiaXgwMH4Lnnsq7EzKwuOUSOpqsrefUpLTOzkhwiR3PGGXDyyQ4RM7MBlBUikhZI2iCpV9LNJaafL2m1pH5JVxZNe1TSHkmPDLDu2yXtKxq7WtI6ST2S7k3HzpC0StKz6fgny9/NCknJKS3foWVmVtKgvbMkjQLuAC4B+oCVkpZGxLqC2bYAi4DPl1jFEmA88Bcl1t0JTCgamwncAsyPiN2STk4nvQy8OyIOSDoOeD6tY9tg+1CVXA5+9CPYswcmTBh8fjOzFlLOkUgX0BsRmyLiIHAfcEXhDBGxOSLWAoeLF46Ix4C9xeNpOC0BbiqadANwR0TsTpffnr4ejIgD6Tzjyqy9evnrIitXjsjmzMwaSTn/EU8BthZ87kvHqnUjsDQiXi4anwXMkvRrSU9JWpCfIGmapLVpPd+o+VEIwLx5yauvi5iZHaGcVvAqMVZVV0JJpwFXARcMUNPMdNpUYIWksyNiT0RsBeakyz8k6cGI+F3RuhcDiwFOP/30aspMTJjgjr5mZgMo50ikD5hW8HkqUO0RwLnADKBX0mZgvKTegu09HBGHIuJFYANJqPxRegTSA7y3eMURcVdEdEZEZ3t7e5VlptzR18yspHJCZCUwU9J0SWOBa4Cl1Ww0IpZFxOSI6IiIDmB/RMxIJz8EXAggaRLJ6a1NkqZKOjYdPxGYTxIwtdfVBdu3w0svjcjmzMwaxaAhEhH9JNcvlgPrgfsjokfSrZIWAkiaJ6mP5BTVnZJ68stLWgE8AFwsqU/SpYNscjmwU9I64HHgCxGxE3gH8LSkNcAvgW9GxMg8Sp7v6OtTWmZmb6Jo4lM0nZ2d0d3dXf2KDh2C44+Hv/xL+Id/qH59ZmZ1TNKqiOgsZ14/sV6OMWPgne/0HVpmZkUcIuXq6oJVq5KjEjMzAxwi5cvl4LXX4Pnns67EzKxuOETK5a/LNTM7gkOkXB0d0N7uEDEzK+AQKZeUXBfxbb5mZn/kEBmKXA7Wr4dXX826EjOzuuAQGYpcLml94o6+ZmaAQ2Ro8h19fUrLzAxwiAzNiSfCrFm+uG5mlnKIDJU7+pqZ/ZFDZKhyOfjtb2Hr1sHnNTNrcg6RoXJHXzOzP3KIDNWcOTBunK+LmJnhEBm6sWPh3HMdImZmOEQqk8slHX37+7OuxMwsUw6RSnR1wf790NMz+LxmZk3MIVIJd/Q1MwMcIpV529tg0iSHiJm1PIdIJdzR18wMcIhUrqsruSayd2/WlZiZZcYhUql8R9/u7qwrMTPLjEOkUl1dyauvi5hZC3OIVGriRJgxw9dFzKylOUSqke/oa2bWohwi1cjlYNs26OvLuhIzs0w4RKrhhw7NrMU5RKpxzjlJQ0ZfFzGzFuUQqca4cTB3ro9EzKxlOUSqlcslz4q8/nrWlZiZjbiyQkTSAkkbJPVKurnE9PMlrZbUL+nKommPStoj6ZEB1n27pH1FY1dLWiepR9K96dhcSU+mY2slfaT83ayhri74wx9g3bqsKzEzG3GDhoikUcAdwGXAbOBaSbOLZtsCLALuLbGKJcBHB1h3JzChaGwmcAswPyLOAj6bTtoPfCwdWwD8o6Q3LZsJX1w3sxZWzpFIF9AbEZsi4iBwH3BF4QwRsTki1gKHixeOiMeAIxpMpeG0BLipaNINwB0RsTtdfnv6+v8iYmP6fhuwHWgvo/7amjEjefDQIWJmLaicEJkCbC343JeOVetGYGlEvFw0PguYJenXkp6StKB4QUldwFjghRLTFkvqltS9Y8eOYShzEPmOvg4RM2tB5YSISoxFNRuVdBpwFXB7icmjgZnABcC1wD2Fp60knQp8B/hvEVHqyOeuiOiMiM729hE6UMl39N23b/B5zcyaSDkh0gdMK/g8FdhW5XbPBWYAvZI2A+Ml9RZs7+GIOBQRLwIbSEIFSccDy4AvRcRTVdYwfHI5OHw4+d51M7MWUk6IrARmSpouaSxwDbC0mo1GxLKImBwRHRHRAeyPiBnp5IeACwEkTSI5vbUp3fYPgf8dEQ9Us/1h546+ZtaiBg2RiOgnuX6xHFgP3B8RPZJulbQQQNI8SX0kp6julNSTX17SCuAB4GJJfZIuHWSTy4GdktYBjwNfiIidwNXA+cAiSc+mP3OHvMe1MGkSvP3tDhEzazmKqOryRl3r7OyM7pH60qg//VNYsQK2bh18XjOzOiZpVUR0ljOvn1gfLrlc0s13W7WXi8zMGodDZLj4oUMza0EOkeEydy6MGeMQMbOW4hAZLscck7SGd1t4M2shDpHhlMvBypXu6GtmLcMhMpxyueSp9fXrs67EzGxEOESGkx86NLMW4xAZTjNnwoQJvi5iZi3DITKc2trc0dfMWopDZLjlcvDcc8m3HZqZNTmHyHDr6nJHXzNrGQ6R4ZZ/ct3XRcysBThEhlt7O0yf7usiZtYSHCK14IvrZtYiHCK1kMslLeFfLv76eDOz5uIQqQVfFzGzFuEQqYVzz4XRo31Ky8yankOkFo49FubMcYiYWdNziNSKO/qaWQtwiNRKLgd798KGDVlXYmZWMw6RWvHX5ZpZC3CI1MqsWXDCCQ4RM2tqDpFaaWuDefMcImbW1BwitZTv6Lt/f9aVmJnVhEOklnK55O6s1auzrsTMrCYcIrXkr8s1sybnEKmlU06BM85wiJhZ03KI1Fou5xAxs6blEKm1XA62bIHf/jbrSszMhl1ZISJpgaQNknol3Vxi+vmSVkvql3Rl0bRHJe2R9MgA675d0r6isaslrZPUI+nectdVl/LXRdzR18ya0KAhImkUcAdwGTAbuFbS7KLZtgCLgHs50hLgowOsuxOYUDQ2E7gFmB8RZwGfLWdddeud74RRo3xKy8yaUjlHIl1Ab0RsioiDwH3AFYUzRMTmiFgLHC5eOCIeA/YWj6fhtAS4qWjSDcAdEbE7XX77YOuqa+PHu6OvmTWtckJkCrC14HNfOlatG4GlEVH89X+zgFmSfi3pKUkLhmFb2erqSjr6Hj4iY83MGlo5IaISY1HNRiWdBlwF3F5i8mhgJnABcC1wj6QJJeYbaN2LJXVL6t6xY0c1ZQ6fXA5efdUdfc2s6ZQTIn3AtILPU4FtVW73XGAG0CtpMzBeUm/B9h6OiEMR8SKwgSRUyhIRd0VEZ0R0tre3V1nmMHFHXzNrUuWEyEpgpqTpksYC1wBLq9loRCyLiMkR0RERHcD+iJiRTn4IuBBA0iSS01ubqtle5s48E976VoeImTWdQUMkIvpJrl8sB9YD90dEj6RbJS0EkDRPUh/JKao7JfXkl5e0AngAuFhSn6RLB9nkcmCnpHXA48AXImJnheuqD/mOvr7N18yajCKqurxR1zo7O6O7uzvrMhJf/CIsWZJcGzn22KyrMTMbkKRVEdFZzrx+Yn2k5HLQ3++OvmbWVBwiI8VPrptZE3KIjJRTT4Vp03xx3cyaikNkJLmjr5k1GYfISMrlYPNm2L590FnNzBqBQ2Qk5R869HURM2sSDpGR5I6+ZtZkHCIj6S1vgbPPdoiYWdNwiIy0XC45neWOvmbWBBwiI62rC37/e9i4MetKzMyq5hAZae7oa2ZNxCEy0t7xDjjuOIeImTUFh8hIGzUq6ejrEDGzJuAQyUJXF6xZA6+9lnUlZmZVcYhkId/R9ze/yboSM7OqOESy4IvrZtYkHCJZOO00mDrVIWJmDc8hkpWuLvfQMrOG5xDJSi4HmzbBjh1ZV2JmVjGHSFbc0dfMmoBDJCvnnQdtbb4uYmYNzSGSleOOg7POcoiYWUNziGQpl4OVKyEi60rMzCriEMlSLge7d7ujr5k1LIdIlvzQoZk1OIdIlmbPTr7t0HdomVmDcohkadQo6Oz0kYiZNSyHSNZyOXj2WXf0NbOG5BDJWi4Hhw4lQWJm1mAcIlnr6kpefV3EzBpQWSEiaYGkDZJ6Jd1cYvr5klZL6pd0ZdG0RyXtkfTIAOu+XdK+orGrJa2T1CPp3oLxj0vamP58vLxdrHNTpyZdfX1dxMwa0OjBZpA0CrgDuAToA1ZKWhoR6wpm2wIsAj5fYhVLgPHAX5RYdycwoWhsJnALMD8idks6OR2fCPwt0AkEsCqtY/dg+1D3cjmHiJk1pHKORLqA3ojYFBEHgfuAKwpniIjNEbEWOFy8cEQ8BuwtHk/DaQlwU9GkG4A78uEQEdvT8UuBn0XErnTaz4AFZdRf/3I5eOEFeOWVrCsxMxuSckJkCrC14HNfOlatG4GlEfFy0fgsYJakX0t6SlI+KMqqQ9JiSd2Sunc0Spv1/EOHK1dmW4eZ2RCVEyIqMVZVsydJpwFXAbeXmDwamAlcAFwL3CNpQrl1RMRdEdEZEZ3t7e3VlDlyzjsPJJ/SMrOGU06I9AHTCj5PBbZVud1zgRlAr6TNwHhJvQXbezgiDkXEi8AGklCpRR314a1vdUdfM2tI5YTISmCmpOmSxgLXAEur2WhELIuIyRHREREdwP6ImJFOfgi4EEDSJJLTW5uA5cD7JZ0o6UTg/elYc8jlktt83dHXzBrIoCESEf0k1y+WA+uB+yOiR9KtkhYCSJonqY/kFNWdknryy0taATwAXCypT9Klg2xyObBT0jrgceALEbEzInYBXyMJtZXArelYc8jlYNeu5AK7mVmDUDTxX76dnZ3R3d2ddRnlWbsWzjkH/u3f4Lrrsq7GzFqYpFUR0VnOvH5ivV7Mng3jx/u6iJk1FIdIvRg92h19zazhOETqSb6j74EDWVdiZlYWh0g9yeXg4EFYsybrSszMyuIQqSf5jr4+pWVmDcIhUk+mToVTT3WImFnDcIjUE+mNhw7NzBqAQ6Te5HKwcWPy4KGZWZ1ziNSbfEdfH42YWQNwiNQbd/Q1swbiEKk3xx+fPL3uIxEzawAOkXqU/7rcJu5rZmbNwSFSj+bPh5074bvfzboSM7OjcojUo+uvh/e9Dz7xCfiP/8i6GjOzATlE6tHYsfCDH0BHB3zoQ9DbO+giZmZZcIjUq4kTYdmy5P3ll/u5ETOrSw6RejZjBjz0EGzeDB/+cNKc0cysjjhE6t173gPf+hb84heweLHv2DKzujI66wKsDNddl1wX+cpXkqOTL30p64rMzACHSOP48peTIPmbv0mC5Jprsq7IzMynsxqGBPfcA+99LyxaBE88kXVFZmYOkYYybhz88IcwbRpccQW88ELWFZlZi3OINJqTTkpu/T18OLn1d/furCsysxbmEGlEs2YlRySbNvnWXzPLlEOkUZ1/fnKN5PHH4ZOf9K2/ZpYJ353VyD72seSOra99DWbOhFtuyboiM2sxDpFG99WvJkHyxS/C298OV1+ddUVm1kJ8OqvRSckT7fPnJ0cmTz2VdUVm1kIcIs3gmGOSC+1TpsDChfDii1lXZGYtoqwQkbRA0gZJvZJuLjH9fEmrJfVLurJo2qOS9kh6ZIB13y5pX8HnRZJ2SHo2/fnzgmnfkPR8+vOR8nezBbS3J7f+HjqU3Pq7Z0/WFZlZCxg0RCSNAu4ALgNmA9dKml002xZgEXBviVUsAT46wLo7gQklJn0/IuamP/ek814OvBOYC+SAL0g6frD6W8qZZybfQ7JxI1x1VRIoZmY1VM6RSBfQGxGbIuIgcB9wReEMEbE5ItYCh4sXjojHgL3F42k4LQFuKrPW2cAvI6I/Iv4ArAEWlLls67jwQrj7bvj5z+FTn/Ktv2ZWU+WEyBRga8HnvnSsWjcCSyPi5RLTPixpraQHJU1Lx9YAl0kaL2kScCEwrXhBSYsldUvq3rFjxzCU2YAWLUru1rr7bvjmN7OuxsyaWDkhohJjVf15K+k04Crg9hKTfwR0RMQc4OfAtwEi4qfAj4EngO8BTwL9RxQWcVdEdEZEZ3t7ezVlNravfQ0+8hH4679OTnGZmdVAOSHSx5v/4p8KbKtyu+cCM4BeSZuB8ZJ6ASJiZ0QcSOe7Gzgvv1BE3JZeJ7mEJNw2VllH82prg3/5F8jl4Prr4Zlnsq7IzJpQOSGyEpgpabqkscA1wNJqNhoRyyJickR0REQHsD8iZgBIOrVg1oXA+nR8lKST0vdzgDnAT6upo+kdeyw8/DBMnpzc+vvSS1lXZGZNZtAQiYh+kusXy0n+Q78/Inok3SppIYCkeZL6SE5R3SmpJ7+8pBXAA8DFkvokXTrIJj8tqUfSGuDTJHd9AYwBVkhaB9wFXJ/WZkdz8snJrb+vvQYf/CD8/vdZV2RmTUTRxHfvdHZ2Rnd3d9Zl1IfHHoMFC+Dii+GRR2C0O96YWWmSVkVEZznz+on1VnHxxXDnnbB8OfzVX/nWXzMbFv5ztJX82Z8lDyJ+/etJ19/PfS7risyswTlEWs1ttyVfq/v5z8Pb3gYf+lDWFZlZA/PprFbT1gbf/jZ0dcF114GvGZlZFRwirSh/6+/JJ8Of/Als2ZJ1RWbWoBwireqUU5Jbf/fvT279ffXVrCsyswbkEGlls2fDgw/CunVJi5R+P3ZjZkPjEGl1l1wC//RP8Oij8JnP+NZfMxsS351lcMMNya2/S5Ykt/5+9rNZV2RmDcIhYomvfz259fdzn0tu/V24MOuKzKwB+HSWJdra4Dvfgc5OuPZaWLky64rMrAE4ROwN48fD0qUwaVLyHMl558GXvwxPPw2vv551dWZWhxwi9maTJ8MTT8Df/V3yPMltt8G73pWMf+xj8P3vw+7dWVdpZnXCXXzt6HbuTJo2/vjH8JOfwK5dMGoUvPvdcPnlyc9ZZ4FKfQGmmTWioXTxdYhY+V5/PTm1tWxZEirPPpuMn346fOADSaBcdFFyWszMGpZDJOUQqbH//M8kTJYtg5//HP7wBzjmGLjwwjeOUjo6sq7SzIbIIZJyiIygAwfgV79KAmXZMujtTcZnz37jKGX+fBgzJts6zWxQDpGUQyRDGze+ESi//CUcOgQnnADvf38SKAsWJP27zKzuOERSDpE6sXdv8vW8+Wsp27Yl4/PmJYHygQ8ktxO3+WZBs3rgEEk5ROpQRHJBPn8t5amnkrFTToHLLktC5ZJLkqMWM8uEQyTlEGkAr7ySNH9ctix53bMHRo+GKVPgpJNg4sTkNf8z0OcJE3wkYzZMHCIph0iD6e+HJ59MnkvZsiV5RmXXruR1587kIceBfl/b2uDEE48eOqXev+UtfsbFrMhQQsQNGK1+jB4N731v8lPK4cPJkUo+VAoDpvjztm3w/PPJ+337Bt7m2LFvBEr+9cQTYdy45E6y0aOT1/xP/nPheKmxgeY52rTCV+mNn7a2N96b1RmHiDWOtrbkP/qJE5OW9eU6cCA5iikVNsXve3uTeQ8eTO4o6+9PXg8dSkKsHgwUMEMdO9o8xdsbjveVLlOO4QzYrMN6uLY/Zw5873vDs66jcIhY8xs3Lun9NXlydes5fPiNUCl+Pdr7Up/zY6WW6e9PTtsV/hw+fOTY0aaVO1ZqnkKFpw+reV/pMuUYzlPyWZ/eH87tT58+fOs6CoeIWbna2pLTX2PHZl2JWd3w7SxmZlYxh4iZmVXMIWJmZhVziJiZWcXKChFJCyRtkNQr6eYS08+XtFpSv6Qri6Y9KmmPpEcGWPftkvYVfF4kaYekZ9OfPy+Y9veSeiStl/S/pKzvxTMza22DhoikUcAdwGXAbOBaSbOLZtsCLALuLbGKJcBHB1h3JzChxKTvR8Tc9OeedN53A/OBOcDZwDzgfYPVb2ZmtVPOkUgX0BsRmyLiIHAfcEXhDBGxOSLWAkc8jRURjwF7i8fTcFoC3FRmrQEcA4wFxgFjgN+VuayZmdVAOSEyBdha8LkvHavWjcDSiHi5xLQPS1or6UFJ0wAi4kngceDl9Gd5RKwvXlDSYkndkrp37NgxDGWamdlAynnYsNR1h6oeq5R0GnAVcEGJyT8CvhcRByR9Evg2cJGkGcA7gKnpfD+TdH5E/OpNhUXcBdyVbmeHpJeqKHUS8EoVyzcy73vrauX9b+V9hzf2/4xyFygnRPqAaQWfpwLbhlbXEc4FZgC96bXx8ZJ6I2JGROwsmO9u4Bvp+/8KPBUR+wAk/QR4F/CmECkUEe3VFCmpu9xOls3G+96a+w6tvf+tvO9Q2f6XczprJTBT0nRJY4FrgKWVFJgXEcsiYnJEdEREB7A/ImYASDq1YNaFQP6U1RbgfZJGSxpDclH9iNNZZmY2cgYNkYjoJ7l+sZzkP+37I6JH0q2SFgJImiepj+QU1Z2SevLLS1oBPABcLKlP0qWDbPLT6W28a4BPk9z1BfAg8ALwHLAGWBMRPxrCvpqZ2TBr6i+lqpakxek1lpbjfW/NfYfW3v9W3neobP8dImZmVjG3PTEzs4o5RMzMrGIOkRIG6xXWbCR9S9J2Sc8XjE2U9DNJG9PXE7OssVYkTZP0eNqPrUfSZ9Lxpt9/ScdIekbSmnTfv5qOT5f0dLrv30/vymxKkkZJ+k2+t1+L7ftmSc+lPQq707Eh/947RIqU2Sus2fwrsKBo7GbgsYiYCTyWfm5G/cB/j4h3kDx39Kn037sV9v8AcFFEnAPMBRZIehfJs1n/M9333cAnMqyx1j7Dmx8VaKV9B7gw7VGYfzZkyL/3DpEjDdorrNmkT/3vKhq+gqRbAOnrh0a0qBESES9HxOr0/V6S/1Cm0AL7H4l8B+0x6U8AF5HcUg9Nuu8AkqYClwP5Jq+iRfb9KIb8e+8QOVKteoU1mlPyfc3S15MzrqfmJHWQdFN4mhbZ//R0zrPAduBnJM9i7UmfD4Pm/v3/R5IGsPnGsSfROvsOyR8MP5W0StLidGzIv/fltD1pNcPeK8zqn6TjgH8HPhsRr7bKV9VExOvAXEkTgB+S9Kc7YraRrar2JH0Q2B4RqyRdkB8uMWvT7XuB+RGxTdLJJL0I/28lK/GRyJFq0SusEf0u34Imfd2ecT01k7bR+XfguxHxg3S4ZfYfICL2AL8guS40QVL+D8xm/f2fDyyUtJnklPVFJEcmrbDvAETEtvR1O8kfEF1U8HvvEDnSsPcKa1BLgY+n7z8OPJxhLTWTngf/Z2B9RPyPgklNv/+S2tMjECQdC/wXkmtCjwP5byhtyn2PiFsiYmrau+8a4P9ExHW0wL4DSHqLpLfm3wPvB56ngt97P7FegqQPkPxVMgr4VkTclnFJNSXpeyRt+SeRfNHX3wIPAfcDp5M0v7wqIoovvjc8Se8BVpD0ZMufG/8iyXWRpt5/SXNILp6OIvmD8v6IuFXS20j+Op8I/Aa4PiIOZFdpbaWnsz4fER9slX1P9/OH6cfRwL0RcZukkxji771DxMzMKubTWWZmVjGHiJmZVcwhYmZmFXOImJlZxRwiZmZWMYeImZlVzCFiZmYV+/9i0fTsjjTsqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.plot_cost_to_epoch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_lrc_to_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(324)\n",
    "testx = np.random.random((3,100))\n",
    "testy = np.random.random((1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169,\n",
       "        0.32200169, 0.32200169, 0.32200169, 0.32200169, 0.32200169]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp = model.predict(testx)\n",
    "yp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71299246, 0.04982137, 0.73016145, 0.25420998, 0.44495708,\n",
       "        0.86496786, 0.82793213, 0.27713124, 0.97564147, 0.08728128,\n",
       "        0.94509777, 0.57506829, 0.24638781, 0.44148505, 0.77545156,\n",
       "        0.19365395, 0.76892746, 0.51696696, 0.97428524, 0.25537007,\n",
       "        0.50956582, 0.40803774, 0.17743865, 0.85696992, 0.12666446,\n",
       "        0.66571726, 0.79701508, 0.04241896, 0.67201997, 0.36202775,\n",
       "        0.20823939, 0.23633635, 0.54961093, 0.68388619, 0.73546364,\n",
       "        0.04760658, 0.0557305 , 0.15886655, 0.5270601 , 0.29028623,\n",
       "        0.24058677, 0.17946958, 0.23064873, 0.54216192, 0.99752858,\n",
       "        0.88929343, 0.62739846, 0.69308132, 0.07062972, 0.78011738,\n",
       "        0.54604046, 0.03684448, 0.15091113, 0.90119243, 0.73694127,\n",
       "        0.65615975, 0.66214813, 0.00311905, 0.74818888, 0.31146595,\n",
       "        0.04914621, 0.71315964, 0.81575319, 0.14434124, 0.74409798,\n",
       "        0.57179619, 0.58861185, 0.74917765, 0.91269009, 0.02548648,\n",
       "        0.49978538, 0.22127859, 0.06628092, 0.72307659, 0.83586941,\n",
       "        0.97439116, 0.05675721, 0.85742472, 0.43639114, 0.2404288 ,\n",
       "        0.04508405, 0.55007715, 0.41001322, 0.07009699, 0.77495059,\n",
       "        0.2872626 , 0.68246056, 0.819129  , 0.04915094, 0.53852277,\n",
       "        0.84347023, 0.12749589, 0.96909821, 0.15261175, 0.19981671,\n",
       "        0.249226  , 0.28513635, 0.09957235, 0.13582372, 0.10325804]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "model.mse_model_eval(testy,yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_model('pydann_model.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \n",
      "        ANN model Help\n",
      "        \n",
      "        Inorder to make an Artificial Neural Network model using pydANN, intanciate the class ann()\n",
      "        model = ann()\n",
      "        Inorder to add hidden layers to the model use the method add_hl( hidden_Layers_list , activations_for_each_layer + 1 )\n",
      "        # we are now going to add hidden layers to the model \n",
      "\n",
      "        model.add_hl( [ 2,5,5 ] , activation_functions = ['relu', 'relu' ,'relu' ,'sigmoid'] )\n",
      "\n",
      "        # the above line adds to the model three hidden layers each of 2, 5 and 5 nodes, respectively. \n",
      "        # The activation_functions should be 1 more in length so as to have one for each hl and an activation for the output layer\n",
      "        To train the model, use the fit( xtrain, ytrain, epoch = 50, learning_rate = 0.01, verbose = 0,decay = True, decay_iter = 5, decay_rate = 0.9, stop_decay_counter = 100, loss_function = 'mse' ) method\n",
      "        # you can also register the training data before you train the model.\n",
      "\n",
      "        model.fit( train_x , y_train )  # this is the basic implementation of the method without any alteration\n",
      "        epoch : The number of iterations to train the model\n",
      "\n",
      "        learning_rate : The value with which the algorithm optimizes weights and biases\n",
      "\n",
      "        verbose : When put 0, verbose is False ( There won't be data printing ) if set to another value other than 0, then the data is printed after each verbose interval\n",
      "\n",
      "        decay : This value optimizes the learning_rate when set to True\n",
      "\n",
      "        loss_function : The function using which the loss is calculated. Loss function option : 'mse' , 'rmse'\n",
      "\n",
      "        decay_rate : The fractional value with which the learning rate is altered\n",
      "\n",
      "        To plot the change in the cost(s) after each epoch, use plot_cost_to_epoch()\n",
      "        model.plot_cost_to_epoch()\n",
      "        To plot the change in learning_rate after each epoch, use plot_lrc_to_epoch()\n",
      "        model.plot_lrc_to_epoch()\n",
      "        To predict on test data, use the function predict( xtest )\n",
      "        y_predictions = model.predict( xtest )\n",
      "        To evaluate the model base on Mean Squared Error, use mse_model_eval( ytest,ypreds )\n",
      "        model.mse_model_eval( ytest,ypreds )\n",
      "        To save the current trained model use method save_model( file = 'pydann_model.dat' )\n",
      "        # 'file' is the name of the file in which the model would be saved and it has to be a .dat file\n",
      "        model.save_model()     # to save the model as pydann_model.dat just call the function like this\n",
      "\n",
      "        # to customize the name of the file in which the model should be saved , change 'file'\n",
      "        model.save_model( file = 'model.dat' )\n",
      "        To use the saved model use method use_model( path )\n",
      "        # path is the file path ( directory ) to the saved model\n",
      "        model.use_model( )\n",
      "        To dispose the model and clear memory use the dispose_model() method\n",
      "        model.dispose_model() \n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "model.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
